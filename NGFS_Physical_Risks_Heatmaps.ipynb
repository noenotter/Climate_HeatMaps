{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985aa271-31d8-4997-b231-ebe5f717d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK 1 OK — robust DAPS selection\n",
      "Rows after filter: 493800\n",
      "Countries retained: 46\n",
      "Sample countries: ['Argentina', 'Austria', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'China', 'Croatia', 'Cyprus', 'Czechia', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany']\n",
      "Hazards present: ['coastal_flood', 'drought', 'floods', 'heatwave', 'river_flood', 'storms', 'tropical_cyclone', 'wildfire', 'winter_storm']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- BLOCK 1 (ROBUST): Load NGFS short-term, normalize country names,\n",
    "#                       keep only direct_impacts, and select the right DAPS per country.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT    = Path.home() / \"Documents\" / \"Summer_intern\"\n",
    "IN_XLSX = ROOT / \"NGFS_shortterm_FULL_timeseries.xlsx\"\n",
    "DAPS_CSV = ROOT / \"chosen_scenarios_by_country.csv\"\n",
    "\n",
    "YEARS = list(range(2023, 2031))\n",
    "FAMILY_SHEETS = [\n",
    "    \"capital_destruction\",\"production_lost\",\"productivity_loss\",\"labour_productivity_loss\"\n",
    "]\n",
    "\n",
    "# --- Load the four sheets (minimal columns) ---\n",
    "frames = []\n",
    "for sheet in FAMILY_SHEETS:\n",
    "    df = pd.read_excel(\n",
    "        IN_XLSX, sheet_name=sheet,\n",
    "        usecols=[\"family\",\"hazard\",\"sector\",\"model\",\"scenario\",\"region\",\"unit\",\"year\",\"value\"]\n",
    "    )\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    if \"family\" not in df.columns:\n",
    "        df[\"family\"] = sheet\n",
    "    frames.append(df)\n",
    "\n",
    "full0 = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Coerce types and basic filters\n",
    "full0[\"year\"]  = pd.to_numeric(full0[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "full0[\"value\"] = pd.to_numeric(full0[\"value\"], errors=\"coerce\")\n",
    "full0 = full0[full0[\"year\"].isin(YEARS)]\n",
    "full0 = full0[full0[\"model\"].fillna(\"\").str.lower().eq(\"direct_impacts\")].copy()\n",
    "\n",
    "# Extract country name from \"Country - ISO3\" (or just \"USA\" etc.)\n",
    "def country_from_region_str(s: str) -> str:\n",
    "    return str(s).split(\" - \")[0].strip()\n",
    "\n",
    "full0[\"country\"] = full0[\"region\"].apply(country_from_region_str)\n",
    "\n",
    "# --- Normalize country labels (so keys match CSVs and plots) ---\n",
    "alias = {\n",
    "    \"United States\": \"USA\",\n",
    "    \"U.S.A.\": \"USA\",\n",
    "    \"US\": \"USA\",\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Korea, Republic of\": \"South Korea\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "}\n",
    "full0[\"country\"] = full0[\"country\"].astype(str).str.strip().replace(alias)\n",
    "\n",
    "# --- Load chosen DAPS per country ---\n",
    "daps = pd.read_csv(DAPS_CSV)\n",
    "daps[\"country\"] = daps[\"country\"].astype(str).str.strip().replace(alias)\n",
    "daps[\"chosen_daps\"] = daps[\"chosen_daps\"].astype(str).str.strip()\n",
    "\n",
    "# Sanity: ensure all countries in data appear in the mapping\n",
    "missing_map = sorted(set(full0[\"country\"]) - set(daps[\"country\"]))\n",
    "if missing_map:\n",
    "    raise RuntimeError(f\"Countries missing in chosen_scenarios_by_country.csv: {missing_map}\")\n",
    "\n",
    "# Tolerant scenario picker: exact -> prefix -> best DAPS by total value\n",
    "def pick_scenario_for_country(df_c, chosen):\n",
    "    # exact\n",
    "    exact = df_c.loc[df_c[\"scenario\"].eq(chosen)]\n",
    "    if not exact.empty:\n",
    "        return exact\n",
    "    # prefix match\n",
    "    pref = df_c.loc[df_c[\"scenario\"].str.startswith(chosen)]\n",
    "    if not pref.empty:\n",
    "        return pref\n",
    "    # fallback: among DAPS_* only, keep the scenario with largest total value\n",
    "    daps_only = df_c.loc[df_c[\"scenario\"].str.startswith(\"DAPS_\")].copy()\n",
    "    if daps_only.empty:\n",
    "        return df_c.iloc[0:0]\n",
    "    scores = (daps_only.groupby(\"scenario\", as_index=False)[\"value\"].sum()\n",
    "                        .sort_values(\"value\", ascending=False))\n",
    "    best = scores[\"scenario\"].iloc[0]\n",
    "    return daps_only.loc[daps_only[\"scenario\"].eq(best)]\n",
    "\n",
    "full0[\"scenario\"] = full0[\"scenario\"].astype(str).str.strip()\n",
    "kept_parts = []\n",
    "for c, dfc in full0.groupby(\"country\", sort=False):\n",
    "    chosen = daps.loc[daps[\"country\"].eq(c), \"chosen_daps\"].iloc[0]\n",
    "    kept = pick_scenario_for_country(dfc, chosen)\n",
    "    if kept.empty:\n",
    "        print(f\"[WARN] No matching DAPS kept for {c} (chosen={chosen}); country dropped\")\n",
    "    else:\n",
    "        kept_parts.append(kept)\n",
    "\n",
    "df_block1 = pd.concat(kept_parts, ignore_index=True)\n",
    "\n",
    "print(\"BLOCK 1 OK — robust DAPS selection\")\n",
    "print(\"Rows after filter:\", len(df_block1))\n",
    "print(\"Countries retained:\", df_block1['country'].nunique())\n",
    "print(\"Sample countries:\", sorted(df_block1['country'].unique().tolist())[:15])\n",
    "print(\"Hazards present:\", sorted(df_block1['hazard'].dropna().unique().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c98a45e-7843-4b6c-8ca6-69688126a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector alias resolution (canonical -> dataset label):\n",
      "            Power Supply  ->  Power Supply\n",
      "          Ferrous metals  ->  Ferrous metals\n",
      "   Non-metallic minerals  ->  Non-metallic minerals\n",
      "            Construction  ->  Construction\n",
      "          Land transport  ->  Land transport\n",
      "         Water transport  ->  Water transport\n",
      "             Warehousing  ->  Warehousing\n",
      "             Agriculture  ->  Agriculture\n",
      "         Market Services  ->  Market Services\n",
      "     Non Market Services  ->  Non Market Services\n",
      "\n",
      "BLOCK 2 OK — sectors filtered\n",
      "Rows after sector filter: 98760\n",
      "Kept sectors (dataset labels, in canonical order): ['Power Supply', 'Ferrous metals', 'Non-metallic minerals', 'Construction', 'Land transport', 'Water transport', 'Warehousing', 'Agriculture', 'Market Services', 'Non Market Services']\n"
     ]
    }
   ],
   "source": [
    "# --- BLOCK 2: lock sector universe to your 10 canonical sectors (with aliases) and filter ---\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = df_block1.copy()\n",
    "\n",
    "# Canonical sector order you requested\n",
    "TARGET_SECTOR_CANON = [\n",
    "    \"Power Supply\",\n",
    "    \"Ferrous metals\",\n",
    "    \"Non-metallic minerals\",\n",
    "    \"Construction\",\n",
    "    \"Land transport\",\n",
    "    \"Water transport\",\n",
    "    \"Warehousing\",\n",
    "    \"Agriculture\",\n",
    "    \"Market Services\",\n",
    "    \"Non Market Services\",\n",
    "]\n",
    "\n",
    "# Alias lists to match dataset labels (be generous but precise)\n",
    "SECTOR_ALIASES = {\n",
    "    \"Power Supply\": [\n",
    "        \"Power Supply\", \"Power supply\", \"Electricity\", \"Electric power\",\n",
    "        \"Electricity and gas\", \"Electricity, gas, steam\", \"Electricity and heat\",\n",
    "        \"Power generation\", \"Electric\", \"Power\"\n",
    "    ],\n",
    "    \"Ferrous metals\": [\n",
    "        \"Ferrous metals\", \"Iron and steel\", \"Basic iron and steel\",\n",
    "        \"Manufacture of basic iron and steel\", \"Basic metals (ferrous)\"\n",
    "    ],\n",
    "    \"Non-metallic minerals\": [\n",
    "        \"Non-metallic minerals\", \"Other non-metallic mineral products\",\n",
    "        \"Cement, lime and plaster\", \"Glass and glass products\",\n",
    "        \"Non metallic minerals\"\n",
    "    ],\n",
    "    \"Construction\": [\"Construction\"],\n",
    "    \"Land transport\": [\"Land transport\"],\n",
    "    \"Water transport\": [\"Water transport\", \"Inland water transport\", \"Water transportation\"],\n",
    "    \"Warehousing\": [\"Warehousing\", \"Warehousing and support activities for transportation\", \"Warehousing and support\"],\n",
    "    \"Agriculture\": [\"Agriculture\"],\n",
    "    \"Market Services\": [\"Market Services\", \"Market services\"],\n",
    "    \"Non Market Services\": [\"Non Market Services\", \"Non-market services\", \"Non Market services\"],\n",
    "}\n",
    "\n",
    "all_sectors = set(df[\"sector\"].dropna().unique())\n",
    "target_map = {}     # canonical -> dataset label actually used\n",
    "missing_targets = []\n",
    "\n",
    "def match_alias(alias_list, universe):\n",
    "    # exact match (case-sensitive then insensitive), else contains-based\n",
    "    for alias in alias_list:\n",
    "        if alias in universe:\n",
    "            return alias\n",
    "        # case-insensitive exact\n",
    "        for s in universe:\n",
    "            if s.lower() == alias.lower():\n",
    "                return s\n",
    "    # contains-based fallback (lowercased)\n",
    "    lowered = {s.lower(): s for s in universe}\n",
    "    for alias in alias_list:\n",
    "        a = alias.lower()\n",
    "        for sl, orig in lowered.items():\n",
    "            if a in sl:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "for canon, aliases in SECTOR_ALIASES.items():\n",
    "    found = match_alias(aliases, all_sectors)\n",
    "    if found is None:\n",
    "        missing_targets.append(canon)\n",
    "    else:\n",
    "        target_map[canon] = found\n",
    "\n",
    "print(\"Sector alias resolution (canonical -> dataset label):\")\n",
    "for k in TARGET_SECTOR_CANON:\n",
    "    print(f\"  {k:>22}  ->  {target_map.get(k, 'NOT FOUND')}\")\n",
    "\n",
    "if missing_targets:\n",
    "    print(\"\\n[WARN] Missing sectors (not found in data):\", missing_targets)\n",
    "    # If strictness is desired, uncomment the next line:\n",
    "    # raise RuntimeError(f\"Missing target sectors: {missing_targets}\")\n",
    "\n",
    "kept_sector_labels = [target_map[s] for s in TARGET_SECTOR_CANON if s in target_map]\n",
    "df = df[df[\"sector\"].isin(kept_sector_labels)].copy()\n",
    "\n",
    "# For later steps, keep a reverse map dataset_label -> canonical name\n",
    "inv_map = {v:k for k,v in target_map.items()}\n",
    "df[\"sector_canon\"] = df[\"sector\"].map(inv_map)\n",
    "\n",
    "print(\"\\nBLOCK 2 OK — sectors filtered\")\n",
    "print(\"Rows after sector filter:\", len(df))\n",
    "print(\"Kept sectors (dataset labels, in canonical order):\", kept_sector_labels)\n",
    "\n",
    "# Expose for next blocks\n",
    "df_block2 = df.copy()\n",
    "target_map_block2 = target_map.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa821513-2f8a-4ce0-b7a4-ded658f88888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK 3 OK\n",
      "     country           sector_canon  peak_storms  peak_flood  peak_heat  \\\n",
      "0  Argentina            Agriculture     0.000000    0.000000   2.049540   \n",
      "1  Argentina           Construction     0.000000    0.000000   4.099081   \n",
      "2  Argentina         Ferrous metals     0.016398    3.519976   0.000000   \n",
      "3  Argentina         Land transport     0.018220    3.911084   0.000000   \n",
      "4  Argentina        Market Services     0.000000    0.000000   0.409908   \n",
      "5  Argentina    Non Market Services     0.000000    0.000000   0.000000   \n",
      "6  Argentina  Non-metallic minerals     0.016398    3.519976   0.000000   \n",
      "7  Argentina           Power Supply     0.018220    3.911084   0.000000   \n",
      "8  Argentina            Warehousing     0.018220    3.911084   0.000000   \n",
      "9  Argentina        Water transport     0.018220    3.911084   0.000000   \n",
      "\n",
      "   peak_drought  peak_wildfire        HDW        SF  \n",
      "0      6.938826       4.754487  13.742853  0.000000  \n",
      "1      0.000000       4.516763   8.615844  0.000000  \n",
      "2      1.387765       0.000000   1.387765  3.536374  \n",
      "3      0.000000       4.754487   4.754487  3.929304  \n",
      "4      0.000000       4.754487   5.164395  0.000000  \n",
      "5      0.000000       4.754487   4.754487  0.000000  \n",
      "6      0.000000       0.000000   0.000000  3.536374  \n",
      "7      0.000000       4.754487   4.754487  3.929304  \n",
      "8      0.000000       4.754487   4.754487  3.929304  \n",
      "9      0.000000       0.000000   0.000000  3.929304  \n"
     ]
    }
   ],
   "source": [
    "# --- BLOCK 3: total_loss per (country, sector_canon, hazard, year)\n",
    "#              + storylines HDW/SF as PEAK (max across years) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = df_block2.copy()\n",
    "\n",
    "# Sum of 4 families (equal weights) per (country, sector_canon, hazard, year)\n",
    "agg = (df.groupby([\"country\",\"sector_canon\",\"hazard\",\"year\"], dropna=False)\n",
    "         .agg(total_loss=(\"value\",\"sum\"))\n",
    "         .reset_index())\n",
    "\n",
    "# Per-year columns for hazards (include tropical_cyclone and winter_storm)\n",
    "need_hazards = [\n",
    "    \"floods\",\"river_flood\",\"coastal_flood\",\n",
    "    \"storms\",\"tropical_cyclone\",\"winter_storm\",\n",
    "    \"heatwave\",\"drought\",\"wildfire\"\n",
    "]\n",
    "\n",
    "base = agg[[\"country\",\"sector_canon\",\"year\"]].drop_duplicates()\n",
    "for h in need_hazards:\n",
    "    if h in agg[\"hazard\"].unique():\n",
    "        sub = (agg[agg[\"hazard\"] == h]\n",
    "               [[\"country\",\"sector_canon\",\"year\",\"total_loss\"]]\n",
    "               .rename(columns={\"total_loss\": f\"tot_{h}\"}))\n",
    "        base = base.merge(sub, on=[\"country\",\"sector_canon\",\"year\"], how=\"left\")\n",
    "    else:\n",
    "        base[f\"tot_{h}\"] = np.nan\n",
    "\n",
    "# Combined flood per year = max of flood components present\n",
    "flood_cols = [c for c in [\"tot_floods\",\"tot_river_flood\",\"tot_coastal_flood\"] if c in base.columns]\n",
    "base[\"tot_flood\"] = (np.nanmax(np.stack([base[c].to_numpy() for c in flood_cols], axis=1), axis=1)\n",
    "                     if flood_cols else np.nan)\n",
    "\n",
    "# Combined storms per year = max(storms, tropical_cyclone, winter_storm) if present\n",
    "storms_cols = [c for c in [\"tot_storms\",\"tot_tropical_cyclone\",\"tot_winter_storm\"] if c in base.columns]\n",
    "base[\"tot_storms_combined\"] = (np.nanmax(np.stack([base[c].to_numpy() for c in storms_cols], axis=1), axis=1)\n",
    "                               if storms_cols else np.nan)\n",
    "\n",
    "# PEAK (max over years) per country–sector for each component\n",
    "peaks = (base.groupby([\"country\",\"sector_canon\"], dropna=False)\n",
    "             .agg(\n",
    "                 peak_storms   = (\"tot_storms_combined\", \"max\"),\n",
    "                 peak_flood    = (\"tot_flood\",           \"max\"),\n",
    "                 peak_heat     = (\"tot_heatwave\",        \"max\"),\n",
    "                 peak_drought  = (\"tot_drought\",         \"max\"),\n",
    "                 peak_wildfire = (\"tot_wildfire\",        \"max\"),\n",
    "             )\n",
    "             .reset_index())\n",
    "\n",
    "# Storylines (sum of component peaks; treat missing as 0)\n",
    "for col in [\"peak_storms\",\"peak_flood\",\"peak_heat\",\"peak_drought\",\"peak_wildfire\"]:\n",
    "    if col not in peaks.columns:\n",
    "        peaks[col] = np.nan\n",
    "\n",
    "peaks[\"HDW\"] = peaks[[\"peak_heat\",\"peak_drought\",\"peak_wildfire\"]].fillna(0.0).sum(axis=1)\n",
    "peaks[\"SF\"]  = peaks[[\"peak_storms\",\"peak_flood\"]].fillna(0.0).sum(axis=1)\n",
    "\n",
    "print(\"BLOCK 3 OK\")\n",
    "print(peaks.head(10))\n",
    "\n",
    "# Expose for next blocks\n",
    "peaks_block3 = peaks.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f41467-9fe6-41ed-a42d-e26fd2a1ead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK 4 OK (hardened)\n",
      "Macro regions present: ['Argentina', 'Australia_NZ', 'Brazil', 'Canada', 'China', 'EU-27', 'Europe_Non_EU', 'India', 'Indonesia', 'Japan', 'Mexico', 'Other', 'Russia', 'South Africa', 'South Korea', 'USA']\n"
     ]
    }
   ],
   "source": [
    "# --- BLOCK 4 (HARDENED): aggregate countries -> macro regions with strong normalization ---\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.home() / \"Documents\" / \"Summer_intern\"\n",
    "MACRO_CSV = ROOT / \"country_to_macro_region.csv\"\n",
    "\n",
    "peaks = peaks_block3.copy()\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    # normalize Unicode, drop NBSP and zero-widths, collapse spaces, strip\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = s.replace(\"\\u00A0\",\" \").replace(\"\\u200B\",\"\").replace(\"\\u200C\",\"\").replace(\"\\u200D\",\"\")\n",
    "    return \" \".join(s.split()).strip()\n",
    "\n",
    "# 1) Normalize countries in peaks (same aliases as Block 1, then strong norm)\n",
    "alias = {\n",
    "    \"United States\": \"USA\",\n",
    "    \"U.S.A.\": \"USA\",\n",
    "    \"US\": \"USA\",\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Korea, Republic of\": \"South Korea\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "}\n",
    "peaks[\"country\"] = peaks[\"country\"].astype(str).map(norm).replace(alias).map(norm)\n",
    "\n",
    "# 2) Load mapping and normalize strongly\n",
    "macro_map = pd.read_csv(MACRO_CSV, dtype=str)\n",
    "if \"country\" not in macro_map.columns or \"macro_region\" not in macro_map.columns:\n",
    "    raise RuntimeError(\"country_to_macro_region.csv must have columns: country, macro_region\")\n",
    "\n",
    "macro_map[\"country\"] = macro_map[\"country\"].astype(str).map(norm).replace(alias).map(norm)\n",
    "macro_map[\"macro_region\"] = macro_map[\"macro_region\"].astype(str).map(norm)\n",
    "\n",
    "# 3) Auto-add essential rows if missing (protects against hidden chars)\n",
    "need_explicit = {\n",
    "    \"USA\": \"USA\",\n",
    "    \"Mexico\": \"Mexico\",\n",
    "    \"Australia\": \"Australia_NZ\",\n",
    "    \"New Zealand\": \"Australia_NZ\",\n",
    "    \"Switzerland\": \"Europe_Non_EU\",\n",
    "    \"Norway\": \"Europe_Non_EU\",\n",
    "    \"Iceland\": \"Europe_Non_EU\",\n",
    "    \"Turkey\": \"Europe_Non_EU\",\n",
    "}\n",
    "missing_essentials = [k for k in need_explicit if not (macro_map[\"country\"] == k).any()]\n",
    "if missing_essentials:\n",
    "    extra = pd.DataFrame({\"country\": missing_essentials,\n",
    "                          \"macro_region\": [need_explicit[k] for k in missing_essentials]})\n",
    "    macro_map = pd.concat([macro_map, extra], ignore_index=True)\n",
    "\n",
    "# 4) Add mapping rows for NGFS aggregate labels (if any appear in your file)\n",
    "ngfs_aggregates = {\n",
    "    \"Oceania\": \"Australia_NZ\",\n",
    "    \"Rest of Europe\": \"Europe_Non_EU\",\n",
    "    \"Rest of World\": \"Other\",\n",
    "    \"Rest of Energy Producing Countries\": \"Other\",\n",
    "}\n",
    "for aggr_country, macro in ngfs_aggregates.items():\n",
    "    if not (macro_map[\"country\"] == aggr_country).any():\n",
    "        macro_map.loc[len(macro_map)] = {\"country\": aggr_country, \"macro_region\": macro}\n",
    "\n",
    "# 5) Drop duplicate countries in the mapping (keep the first occurrence)\n",
    "macro_map = macro_map.drop_duplicates(subset=[\"country\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# 6) Final sanity: make sure every country in peaks is mappable\n",
    "present = set(macro_map[\"country\"])\n",
    "needed  = set(peaks[\"country\"])\n",
    "still_missing = sorted(needed - present)\n",
    "if still_missing:\n",
    "    # As a last resort, auto-assign to \"Other\" to avoid breaking the pipeline\n",
    "    extra = pd.DataFrame({\"country\": still_missing, \"macro_region\": [\"Other\"] * len(still_missing)})\n",
    "    macro_map = pd.concat([macro_map, extra], ignore_index=True)\n",
    "\n",
    "# 7) Merge and aggregate\n",
    "peaks = peaks.merge(macro_map, on=\"country\", how=\"left\", validate=\"many_to_one\")\n",
    "if peaks[\"macro_region\"].isna().any():\n",
    "    bad = sorted(peaks.loc[peaks[\"macro_region\"].isna(), \"country\"].unique())\n",
    "    raise RuntimeError(f\"Unmapped after merge even after normalization: {bad}\")\n",
    "\n",
    "macro = (\n",
    "    peaks.groupby([\"macro_region\",\"sector_canon\"], dropna=False)\n",
    "         .agg(HDW=(\"HDW\",\"sum\"), SF=(\"SF\",\"sum\"))\n",
    "         .reset_index()\n",
    ")\n",
    "\n",
    "# --- Replace 'Other' with MEAN across its member countries (consistency with EU means) ---\n",
    "# Use `peaks`, which already includes `macro_region` from step 7.\n",
    "other_mean = (peaks.loc[peaks[\"macro_region\"] == \"Other\"]\n",
    "              .groupby([\"sector_canon\"], dropna=False)\n",
    "              .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\"))\n",
    "              .reset_index())\n",
    "\n",
    "if not other_mean.empty:\n",
    "    other_mean.insert(0, \"macro_region\", \"Other\")\n",
    "    macro = pd.concat([macro[macro[\"macro_region\"] != \"Other\"], other_mean], ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"BLOCK 4 OK (hardened)\")\n",
    "print(\"Macro regions present:\", sorted(macro[\"macro_region\"].unique().tolist()))\n",
    "\n",
    "macro_block4 = macro.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e5ab97-bbad-47e2-9fc5-8e0c179f8e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK 4A OK — EU-15 and mean EU-27 added where possible.\n",
      "Macro regions now: ['Argentina', 'Australia_NZ', 'Brazil', 'Canada', 'China', 'EU-15', 'EU-27', 'Europe_Non_EU', 'India', 'Indonesia', 'Japan', 'Mexico', 'Other', 'Russia', 'South Africa', 'South Korea', 'USA']\n"
     ]
    }
   ],
   "source": [
    "# --- BLOCK 4A: add EU-15 and EU-27 (means across member countries) ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "peaks_c = peaks_block3.copy()  # raw country-level peaks from Block 3\n",
    "\n",
    "# Robust country labels (match what appears in df_block1/peaks)\n",
    "# EU-15 = pre-2004 members (historical definition, includes UK)\n",
    "EU15 = [\n",
    "    \"Austria\",\"Belgium\",\"Denmark\",\"Finland\",\"France\",\"Germany\",\"Greece\",\n",
    "    \"Ireland\",\"Italy\",\"Luxembourg\",\"Netherlands\",\"Portugal\",\"Spain\",\"Sweden\",\n",
    "    \"United Kingdom\"\n",
    "]\n",
    "\n",
    "# EU-27 current members (2007+; exclude UK, include the 27 today)\n",
    "EU27 = [\n",
    "    \"Austria\",\"Belgium\",\"Bulgaria\",\"Croatia\",\"Cyprus\",\"Czechia\",\"Denmark\",\n",
    "    \"Estonia\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Hungary\",\"Ireland\",\n",
    "    \"Italy\",\"Latvia\",\"Lithuania\",\"Luxembourg\",\"Malta\",\"Netherlands\",\"Poland\",\n",
    "    \"Portugal\",\"Romania\",\"Slovakia\",\"Slovenia\",\"Spain\",\"Sweden\"\n",
    "]\n",
    "\n",
    "def add_macro_mean(peaks_df, members, macro_name):\n",
    "    present = set(peaks_df[\"country\"])\n",
    "    used = [c for c in members if c in present]\n",
    "    if not used:\n",
    "        return None\n",
    "    sub = peaks_df[peaks_df[\"country\"].isin(used)]\n",
    "    # mean across countries for each sector\n",
    "    agg_mean = (sub.groupby([\"sector_canon\"], dropna=False)\n",
    "                  .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\"))\n",
    "                  .reset_index())\n",
    "    agg_mean.insert(0, \"macro_region\", macro_name)\n",
    "    return agg_mean\n",
    "\n",
    "eu15_rows = add_macro_mean(peaks_c, EU15, \"EU-15\")\n",
    "eu27_rows = add_macro_mean(peaks_c, EU27, \"EU-27\")\n",
    "\n",
    "# Start from the macro produced in Block 4 (which used sum by mapping)\n",
    "macro_aug = macro.copy()\n",
    "\n",
    "# If EU-27 already exists from the mapping (sum), drop it and replace by mean\n",
    "if \"EU-27\" in macro_aug[\"macro_region\"].unique():\n",
    "    macro_aug = macro_aug[macro_aug[\"macro_region\"] != \"EU-27\"]\n",
    "\n",
    "# Append the new aggregates if available\n",
    "to_add = []\n",
    "if eu15_rows is not None: to_add.append(eu15_rows)\n",
    "if eu27_rows is not None: to_add.append(eu27_rows)\n",
    "if to_add:\n",
    "    macro_aug = pd.concat([macro_aug] + to_add, ignore_index=True)\n",
    "\n",
    "# Expose for Block 5\n",
    "macro_block4 = macro_aug.copy()\n",
    "print(\"BLOCK 4A OK — EU-15 and mean EU-27 added where possible.\")\n",
    "print(\"Macro regions now:\", sorted(macro_block4[\"macro_region\"].unique()))\n",
    "\n",
    "# DAPS\n",
    "macro_daps = macro_block4.copy()   # this is the one with EU-15/EU-27 means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb15bb9-7430-4278-a4f2-4530aa7306b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK 5 OK\n",
      "PNG saved: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/png/total_loss__HDW__regionsxsectors__peak.png\n",
      "PNG saved: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/png/total_loss__SF__regionsxsectors__peak.png\n",
      "PDF saved: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/pdf/total_loss__HDW_and_SF__regionsxsectors__peak.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- BLOCK 5: plot HDW and SF heatmaps (Regions x Sectors), export PNG + PDF ---\n",
    "\n",
    "\n",
    "\n",
    "# ------- shared CSV helpers (place once near top of file) -------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _to_long_from_matrix(mat_df: pd.DataFrame, index_name=\"Region\", col_name=\"Sector\", value_name=\"Value\"):\n",
    "    idx = mat_df.index.name or index_name\n",
    "    return (mat_df.reset_index()\n",
    "            .rename(columns={idx: index_name})\n",
    "            .melt(id_vars=index_name, var_name=col_name, value_name=value_name))\n",
    "\n",
    "def write_heatmap_tables(base_outdir: Path, tag: str, mat_df: pd.DataFrame):\n",
    "    \"\"\"Write wide + long CSVs next to your PNG/PDF outputs.\"\"\"\n",
    "    data_dir = _ensure_dir(base_outdir / \"data\")\n",
    "    (data_dir / f\"{tag}_heatmap_data.csv\").write_text(mat_df.to_csv())\n",
    "    _to_long_from_matrix(mat_df).to_csv(data_dir / f\"{tag}_heatmap_data_long.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT    = Path.home() / \"Documents\" / \"Summer_intern\"\n",
    "OUTROOT = ROOT / \"outputs\" / \"physical_shortterm_DAPS\"\n",
    "OUTPNG  = OUTROOT / \"png\"\n",
    "OUTPDF  = OUTROOT / \"pdf\"\n",
    "OUTPNG.mkdir(parents=True, exist_ok=True)\n",
    "OUTPDF.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "macro = macro_block4.copy()\n",
    "\n",
    "# Fixed row order = same as transition + Indonesia (and optionally \"Other\" last if present)\n",
    "region_order = [\n",
    "    \"USA\",\"EU-15\",\"EU-27\",\"Europe_Non_EU\",\"Japan\",\"China\",\"India\",\"South Korea\",\n",
    "    \"Canada\",\"Australia_NZ\",\"Mexico\",\"Russia\",\"Brazil\",\n",
    "    \"South Africa\",\"Argentina\",\"Indonesia\",\"Other\"\n",
    "]\n",
    "region_order = [r for r in region_order if r in set(macro_block4[\"macro_region\"])]\n",
    "\n",
    "\n",
    "# Fixed column order = your 10 canonical sectors\n",
    "sector_order = [\n",
    "    \"Power Supply\",\"Ferrous metals\",\"Non-metallic minerals\",\"Construction\",\n",
    "    \"Land transport\",\"Water transport\",\"Warehousing\",\"Agriculture\",\n",
    "    \"Market Services\",\"Non Market Services\",\n",
    "]\n",
    "sector_order = [s for s in sector_order if s in set(macro[\"sector_canon\"])]\n",
    "\n",
    "def matrix_from(df, value_col):\n",
    "    p = df.pivot_table(index=\"macro_region\", columns=\"sector_canon\", values=value_col, aggfunc=\"sum\")\n",
    "    p = p.reindex(index=region_order, columns=sector_order)\n",
    "    return p\n",
    "\n",
    "M_HDW = matrix_from(macro, \"HDW\")\n",
    "M_SF  = matrix_from(macro, \"SF\")\n",
    "\n",
    "# Save CSVs (wide + long) so Streamlit can render + download\n",
    "_ensure_dir(OUTROOT / \"data\")\n",
    "write_heatmap_tables(OUTROOT, \"HDW_regionsxsectors_peak\", M_HDW)\n",
    "write_heatmap_tables(OUTROOT, \"SF_regionsxsectors_peak\",  M_SF)\n",
    "\n",
    "\n",
    "def p95(M):\n",
    "    z = M[np.isfinite(M)]\n",
    "    return float(np.percentile(z,95)) if z.size else 1.0\n",
    "\n",
    "def draw(ax, M, rows, cols, title, unit=\"percent\"):\n",
    "    if M.size == 0:\n",
    "        ax.axis(\"off\"); ax.set_title(f\"{title}\\n(no data)\"); return\n",
    "    vmin, vmax = 0.0, p95(M)\n",
    "\n",
    "    # heatmap\n",
    "    im = ax.imshow(M, aspect=\"auto\", cmap=\"RdYlGn_r\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # ticks and labels\n",
    "    ax.set_xticks(range(len(cols))); ax.set_xticklabels(cols, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(rows))); ax.set_yticklabels(rows)\n",
    "\n",
    "    # cadrillage: put minor ticks at cell boundaries and draw a grid on them\n",
    "    nrows, ncols = M.shape\n",
    "    ax.set_xticks(np.arange(-0.5, ncols, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, nrows, 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linewidth=0.4, linestyle=\"-\")\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    # colorbar with simple formatting\n",
    "    c = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, format=FuncFormatter(lambda v, _pos: f\"{v:.1f}\"))\n",
    "    c.ax.set_ylabel(f\"Exposure ({unit})\", rotation=270, va=\"bottom\")\n",
    "\n",
    "    ax.set_title(title, pad=10)\n",
    "\n",
    "# HDW\n",
    "fig1 = plt.figure(figsize=(12, max(5.0, 0.36*len(M_HDW.index)+2.2)))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "draw(ax1, M_HDW.to_numpy(float), list(M_HDW.index), list(M_HDW.columns),\n",
    "     \"Short-term physical risk (PEAK) DAPS — HDW (heatwave + drought + wildfire)\")\n",
    "fig1.tight_layout()\n",
    "png_hdw = OUTPNG / \"total_loss__HDW__regionsxsectors__peak.png\"\n",
    "fig1.savefig(png_hdw, dpi=240); plt.close(fig1)\n",
    "\n",
    "# SF\n",
    "fig2 = plt.figure(figsize=(12, max(5.0, 0.36*len(M_SF.index)+2.2)))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "draw(ax2, M_SF.to_numpy(float), list(M_SF.index), list(M_SF.columns),\n",
    "     \"Short-term physical risk (PEAK) DAPS — SF (storms + flood)\")\n",
    "fig2.tight_layout()\n",
    "png_sf = OUTPNG / \"total_loss__SF__regionsxsectors__peak.png\"\n",
    "fig2.savefig(png_sf, dpi=240); plt.close(fig2)\n",
    "\n",
    "# Bundle into one PDF\n",
    "pdf = OUTPDF / \"total_loss__HDW_and_SF__regionsxsectors__peak.pdf\"\n",
    "with PdfPages(pdf) as pp:\n",
    "    for png in [png_hdw, png_sf]:\n",
    "        img = plt.imread(png)\n",
    "        fig = plt.figure(figsize=(12,9)); plt.imshow(img); plt.axis('off')\n",
    "        pp.savefig(fig); plt.close(fig)\n",
    "\n",
    "print(\"BLOCK 5 OK\")\n",
    "print(\"PNG saved:\", png_hdw)\n",
    "print(\"PNG saved:\", png_sf)\n",
    "print(\"PDF saved:\", pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f77fdea-e4de-40b4-8f97-243cda3c5d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIRE] PNG saved: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/png/total_loss__HDW__regionsxsectors__peak__DIRE.png\n",
      "[DIRE] PNG saved: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/png/total_loss__SF__regionsxsectors__peak__DIRE.png\n",
      "[DIRE] PDF saved: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/pdf/total_loss__HDW_and_SF__regionsxsectors__peak__DIRE.pdf\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# PHYSICAL RISK — DIRE (DiRe) PARALLEL PIPELINE\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# ---- Paths (keep aligned with your DAPS setup) ----\n",
    "ROOT      = Path.home() / \"Documents\" / \"Summer_intern\"\n",
    "IN_XLSX   = ROOT / \"NGFS_shortterm_FULL_timeseries.xlsx\"\n",
    "MACRO_CSV = ROOT / \"country_to_macro_region.csv\"\n",
    "OUTROOT   = ROOT / \"outputs\" / \"physical_shortterm_DIRE\"\n",
    "OUTPNG    = OUTROOT / \"png\"\n",
    "OUTPDF    = OUTROOT / \"pdf\"\n",
    "OUTPNG.mkdir(parents=True, exist_ok=True)\n",
    "OUTPDF.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "YEARS = list(range(2023, 2031))\n",
    "FAMILY_SHEETS = [\"capital_destruction\",\"production_lost\",\"productivity_loss\",\"labour_productivity_loss\"]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = s.replace(\"\\u00A0\",\" \").replace(\"\\u200B\",\"\").replace(\"\\u200C\",\"\").replace(\"\\u200D\",\"\")\n",
    "    return \" \".join(s.split()).strip()\n",
    "\n",
    "alias = {\n",
    "    \"United States\":\"USA\",\"U.S.A.\":\"USA\",\"US\":\"USA\",\"United States of America\":\"USA\",\n",
    "    \"Korea, Republic of\":\"South Korea\",\"Republic of Korea\":\"South Korea\",\n",
    "    \"Russian Federation\":\"Russia\",\"Viet Nam\":\"Vietnam\",\n",
    "}\n",
    "\n",
    "def _build_full0():\n",
    "    frames = []\n",
    "    for sheet in FAMILY_SHEETS:\n",
    "        df = pd.read_excel(\n",
    "            IN_XLSX, sheet_name=sheet,\n",
    "            usecols=[\"family\",\"hazard\",\"sector\",\"model\",\"scenario\",\"region\",\"unit\",\"year\",\"value\"]\n",
    "        )\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        if \"family\" not in df.columns:\n",
    "            df[\"family\"] = sheet\n",
    "        frames.append(df)\n",
    "    f0 = pd.concat(frames, ignore_index=True)\n",
    "    f0[\"year\"]  = pd.to_numeric(f0[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    f0[\"value\"] = pd.to_numeric(f0[\"value\"], errors=\"coerce\")\n",
    "    f0 = f0[f0[\"year\"].isin(YEARS)]\n",
    "    f0 = f0[f0[\"model\"].fillna(\"\").str.lower().eq(\"direct_impacts\")].copy()\n",
    "    # country from \"Name - ISO3\"\n",
    "    f0[\"country\"] = f0[\"region\"].astype(str).str.split(\" - \").str[0].str.strip()\n",
    "    f0[\"country\"] = f0[\"country\"].replace(alias).map(_norm)\n",
    "    return f0\n",
    "\n",
    "# Reuse DAPS Block-1 output if present; else rebuild\n",
    "if \"full0\" not in globals():\n",
    "    full0 = _build_full0()\n",
    "\n",
    "# -----------------------\n",
    "# BLOCK 1 — Filter DiRe (case-insensitive)\n",
    "# -----------------------\n",
    "if \"scenario\" not in full0.columns:\n",
    "    raise RuntimeError(\"Expected 'scenario' in full0. Did the load block run?\")\n",
    "scenario_norm = full0[\"scenario\"].astype(str).str.strip().str.lower()\n",
    "df = full0.loc[scenario_norm.eq(\"dire\")].copy()\n",
    "if df.empty:\n",
    "    # very tolerant fallback\n",
    "    df = full0.loc[full0[\"scenario\"].astype(str).str.lower().str.contains(\"dire\")].copy()\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"No rows for DiRe/DIRE scenario found in the workbook.\")\n",
    "\n",
    "# -----------------------\n",
    "# BLOCK 2 — Map to the 10 canonical sectors (same as your DAPS)\n",
    "# -----------------------\n",
    "TARGET_SECTOR_CANON = [\n",
    "    \"Power Supply\",\"Ferrous metals\",\"Non-metallic minerals\",\"Construction\",\n",
    "    \"Land transport\",\"Water transport\",\"Warehousing\",\"Agriculture\",\n",
    "    \"Market Services\",\"Non Market Services\",\n",
    "]\n",
    "SECTOR_ALIASES = {\n",
    "    \"Power Supply\": [\"Power Supply\",\"Power supply\",\"Electricity\",\"Electric power\",\"Electric\"],\n",
    "    \"Ferrous metals\": [\"Ferrous metals\",\"Iron and steel\",\"Basic iron and steel\"],\n",
    "    \"Non-metallic minerals\": [\"Non-metallic minerals\",\"Other non-metallic\",\"Cement\",\"Glass\"],\n",
    "    \"Construction\": [\"Construction\"],\n",
    "    \"Land transport\": [\"Land transport\"],\n",
    "    \"Water transport\": [\"Water transport\",\"Inland water transport\",\"Water transportation\"],\n",
    "    \"Warehousing\": [\"Warehousing\",\"Warehousing and support activities for transportation\",\"Warehousing and support\"],\n",
    "    \"Agriculture\": [\"Agriculture\"],\n",
    "    \"Market Services\": [\"Market Services\",\"Market services\"],\n",
    "    \"Non Market Services\": [\"Non Market Services\",\"Non-market services\",\"Non Market services\"],\n",
    "}\n",
    "\n",
    "all_sectors = set(df[\"sector\"].dropna().astype(str).unique())\n",
    "def _match_alias(alias_list, universe):\n",
    "    for a in alias_list:\n",
    "        if a in universe: return a\n",
    "    lower_index = {s.lower(): s for s in universe}\n",
    "    for a in alias_list:\n",
    "        if a.lower() in lower_index: return lower_index[a.lower()]\n",
    "    for a in alias_list:\n",
    "        a_low = a.lower()\n",
    "        for s in universe:\n",
    "            if a_low in s.lower(): return s\n",
    "    return None\n",
    "\n",
    "target_map = {}\n",
    "for canon, aliases in SECTOR_ALIASES.items():\n",
    "    found = _match_alias(aliases, all_sectors)\n",
    "    if found: target_map[canon] = found\n",
    "\n",
    "if not target_map:\n",
    "    raise RuntimeError(\"Could not map any sectors. Inspect df['sector'].unique().\")\n",
    "\n",
    "df = df[df[\"sector\"].isin(target_map.values())].copy()\n",
    "inv_map = {v:k for k,v in target_map.items()}\n",
    "df[\"sector_canon\"] = df[\"sector\"].map(inv_map)\n",
    "\n",
    "# -----------------------\n",
    "# BLOCK 3 — Total loss per country×sector×hazard×year; take PEAKs; build HDW & SF\n",
    "# -----------------------\n",
    "agg = (df.groupby([\"country\",\"sector_canon\",\"hazard\",\"year\"], dropna=False)\n",
    "         .agg(total_loss=(\"value\",\"sum\")).reset_index())\n",
    "\n",
    "base = agg[[\"country\",\"sector_canon\",\"year\"]].drop_duplicates()\n",
    "\n",
    "hazards = [\"coastal_flood\",\"river_flood\",\"floods\",\"storms\",\"tropical_cyclone\",\"winter_storm\",\n",
    "           \"heatwave\",\"drought\",\"wildfire\"]\n",
    "for hz in hazards:\n",
    "    sub = agg.loc[agg[\"hazard\"].eq(hz), [\"country\",\"sector_canon\",\"year\",\"total_loss\"]]\n",
    "    base = base.merge(sub.rename(columns={\"total_loss\": f\"tot_{hz}\"}),\n",
    "                      on=[\"country\",\"sector_canon\",\"year\"], how=\"left\")\n",
    "\n",
    "def _rowmax(df_cols):\n",
    "    if not df_cols: return np.full(len(base), np.nan)\n",
    "    arr = np.stack([base[c].to_numpy() for c in df_cols], axis=1)\n",
    "    return np.nanmax(arr, axis=1)\n",
    "\n",
    "flood_cols  = [c for c in [\"tot_floods\",\"tot_river_flood\",\"tot_coastal_flood\"] if c in base.columns]\n",
    "storm_cols  = [c for c in [\"tot_storms\",\"tot_tropical_cyclone\",\"tot_winter_storm\"] if c in base.columns]\n",
    "base[\"tot_flood\"]   = _rowmax(flood_cols)\n",
    "base[\"tot_storms\"]  = _rowmax(storm_cols)\n",
    "\n",
    "peaks = (base.groupby([\"country\",\"sector_canon\"], dropna=False)\n",
    "            .agg(peak_heat=(\"tot_heatwave\",\"max\"),\n",
    "                 peak_drought=(\"tot_drought\",\"max\"),\n",
    "                 peak_wildfire=(\"tot_wildfire\",\"max\"),\n",
    "                 peak_flood=(\"tot_flood\",\"max\"),\n",
    "                 peak_storms=(\"tot_storms\",\"max\"))\n",
    "            .reset_index())\n",
    "\n",
    "peaks[\"HDW\"] = peaks[[\"peak_heat\",\"peak_drought\",\"peak_wildfire\"]].fillna(0).sum(axis=1)\n",
    "peaks[\"SF\"]  = peaks[[\"peak_storms\",\"peak_flood\"]].fillna(0).sum(axis=1)\n",
    "\n",
    "# -----------------------\n",
    "# BLOCK 4 — Country→Macro mapping (robust), then EU-15 & EU-27 means\n",
    "# -----------------------\n",
    "def _safe_norm(x):\n",
    "    return _norm(x) if isinstance(x, str) else x\n",
    "\n",
    "macro_map = pd.read_csv(MACRO_CSV, dtype=str)\n",
    "macro_map[\"country\"]      = macro_map[\"country\"].map(_safe_norm).replace(alias).map(_safe_norm)\n",
    "macro_map[\"macro_region\"] = macro_map[\"macro_region\"].map(_safe_norm)\n",
    "\n",
    "# Add robust defaults for NGFS short-term buckets + basics if missing\n",
    "need_explicit = {\n",
    "    \"USA\": \"USA\",\n",
    "    \"Mexico\": \"Mexico\",\n",
    "    \"Oceania\": \"Australia_NZ\",\n",
    "    \"Rest of Europe\": \"Europe_Non_EU\",\n",
    "    \"Rest of Energy Producing Countries\": \"Other\",\n",
    "    \"Rest of World\": \"Other\",\n",
    "}\n",
    "missing_rows = [k for k in need_explicit if not (macro_map[\"country\"] == k).any()]\n",
    "if missing_rows:\n",
    "    macro_map = pd.concat(\n",
    "        [macro_map,\n",
    "         pd.DataFrame({\n",
    "             \"country\": missing_rows,\n",
    "             \"macro_region\": [need_explicit[k] for k in missing_rows]\n",
    "         })],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "macro_map = macro_map.drop_duplicates(subset=[\"country\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "peaks[\"country\"] = peaks[\"country\"].map(_safe_norm)\n",
    "peaks = peaks.merge(macro_map, on=\"country\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "# final catch-all: still-unmapped → 'Other' (and report)\n",
    "if peaks[\"macro_region\"].isna().any():\n",
    "    still_missing = sorted(peaks.loc[peaks[\"macro_region\"].isna(), \"country\"].unique())\n",
    "    print(\"Auto-mapping remaining to 'Other':\", still_missing)\n",
    "    peaks.loc[peaks[\"macro_region\"].isna(), \"macro_region\"] = \"Other\"\n",
    "\n",
    "# Sum to macro for all macros\n",
    "macro = (peaks.groupby([\"macro_region\",\"sector_canon\"], dropna=False)\n",
    "              .agg(HDW=(\"HDW\",\"sum\"), SF=(\"SF\",\"sum\")).reset_index())\n",
    "\n",
    "# --- Replace 'Other' with MEAN across its member countries (consistency with EU means) ---\n",
    "# Use `peaks`, which already includes `macro_region` from step 7.\n",
    "other_mean = (peaks.loc[peaks[\"macro_region\"] == \"Other\"]\n",
    "              .groupby([\"sector_canon\"], dropna=False)\n",
    "              .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\"))\n",
    "              .reset_index())\n",
    "\n",
    "if not other_mean.empty:\n",
    "    other_mean.insert(0, \"macro_region\", \"Other\")\n",
    "    macro = pd.concat([macro[macro[\"macro_region\"] != \"Other\"], other_mean], ignore_index=True)\n",
    "\n",
    "\n",
    "# EU-means from country-level peaks (avoid inflation)\n",
    "EU15 = [\"Austria\",\"Belgium\",\"Denmark\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Ireland\",\"Italy\",\n",
    "        \"Luxembourg\",\"Netherlands\",\"Portugal\",\"Spain\",\"Sweden\",\"United Kingdom\"]\n",
    "EU27 = [\"Austria\",\"Belgium\",\"Bulgaria\",\"Croatia\",\"Cyprus\",\"Czechia\",\"Denmark\",\"Estonia\",\"Finland\",\n",
    "        \"France\",\"Germany\",\"Greece\",\"Hungary\",\"Ireland\",\"Italy\",\"Latvia\",\"Lithuania\",\"Luxembourg\",\n",
    "        \"Malta\",\"Netherlands\",\"Poland\",\"Portugal\",\"Romania\",\"Slovakia\",\"Slovenia\",\"Spain\",\"Sweden\"]\n",
    "\n",
    "def _eu_mean(peaks_df, members, name):\n",
    "    used = [c for c in members if c in set(peaks_df[\"country\"])]\n",
    "    if not used: return None\n",
    "    out = (peaks_df[peaks_df[\"country\"].isin(used)]\n",
    "           .groupby([\"sector_canon\"], dropna=False)\n",
    "           .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\")).reset_index())\n",
    "    out.insert(0, \"macro_region\", name)\n",
    "    return out\n",
    "\n",
    "eu15_rows = _eu_mean(peaks, EU15, \"EU-15\")\n",
    "eu27_rows = _eu_mean(peaks, EU27, \"EU-27\")\n",
    "\n",
    "macro_aug = macro.copy()\n",
    "if \"EU-27\" in macro_aug[\"macro_region\"].unique():\n",
    "    macro_aug = macro_aug[macro_aug[\"macro_region\"] != \"EU-27\"]\n",
    "for extra in [eu15_rows, eu27_rows]:\n",
    "    if extra is not None:\n",
    "        macro_aug = pd.concat([macro_aug, extra], ignore_index=True)\n",
    "        \n",
    "# DiRe\n",
    "macro_dire = macro_aug.copy()\n",
    "\n",
    "# -----------------------\n",
    "# BLOCK 5 — Heatmaps (same style & grid as DAPS)\n",
    "# -----------------------\n",
    "def _matrix(df, value_col, region_order, sector_order):\n",
    "    p = df.pivot_table(index=\"macro_region\", columns=\"sector_canon\", values=value_col, aggfunc=\"sum\")\n",
    "    p = p.reindex(index=[r for r in region_order if r in p.index],\n",
    "                  columns=[s for s in sector_order if s in p.columns])\n",
    "    return p\n",
    "\n",
    "region_order = [\n",
    "    \"USA\",\"EU-15\",\"EU-27\",\"Europe_Non_EU\",\"Japan\",\"China\",\"India\",\"South Korea\",\n",
    "    \"Canada\",\"Australia_NZ\",\"Mexico\",\"Russia\",\"Brazil\",\n",
    "    \"South Africa\",\"Argentina\",\"Indonesia\",\"Other\"\n",
    "]\n",
    "sector_order = [\n",
    "    \"Power Supply\",\"Ferrous metals\",\"Non-metallic minerals\",\"Construction\",\n",
    "    \"Land transport\",\"Water transport\",\"Warehousing\",\"Agriculture\",\n",
    "    \"Market Services\",\"Non Market Services\",\n",
    "]\n",
    "\n",
    "M_HDW = _matrix(macro_aug, \"HDW\", region_order, sector_order)\n",
    "M_SF  = _matrix(macro_aug, \"SF\",  region_order, sector_order)\n",
    "\n",
    "_ensure_dir(OUTROOT / \"data\")\n",
    "write_heatmap_tables(OUTROOT, \"HDW_regionsxsectors_peak__DIRE\", M_HDW)\n",
    "write_heatmap_tables(OUTROOT, \"SF_regionsxsectors_peak__DIRE\",  M_SF)\n",
    "\n",
    "\n",
    "def _p95(M):\n",
    "    z = M.to_numpy(dtype=float)\n",
    "    z = z[np.isfinite(z)]\n",
    "    return float(np.percentile(z,95)) if z.size else 1.0\n",
    "\n",
    "def _draw(ax, M, rows, cols, title, unit=\"percent\"):\n",
    "    if M.size == 0:\n",
    "        ax.axis(\"off\"); ax.set_title(f\"{title}\\n(no data)\"); return\n",
    "    vmin, vmax = 0.0, _p95(M)\n",
    "    im = ax.imshow(M, aspect=\"auto\", cmap=\"RdYlGn_r\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_xticks(range(len(cols))); ax.set_xticklabels(cols, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(rows))); ax.set_yticklabels(rows)\n",
    "    # “cadrillage”\n",
    "    nrows, ncols = M.shape\n",
    "    ax.set_xticks(np.arange(-0.5, ncols, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, nrows, 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linewidth=0.4, linestyle=\"-\")\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    c = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04,\n",
    "                     format=FuncFormatter(lambda v, _pos: f\"{v:.1f}\"))\n",
    "    c.ax.set_ylabel(f\"Exposure ({unit})\", rotation=270, va=\"bottom\")\n",
    "    ax.set_title(title, pad=10)\n",
    "\n",
    "# HDW\n",
    "fig1 = plt.figure(figsize=(12, max(5.0, 0.36*len(M_HDW.index)+2.2)))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "_draw(ax1, M_HDW, list(M_HDW.index), list(M_HDW.columns),\n",
    "      \"Short-term physical risk (PEAK) — DIRE / HDW (heatwave + drought + wildfire)\")\n",
    "fig1.tight_layout()\n",
    "png_hdw = OUTPNG / \"total_loss__HDW__regionsxsectors__peak__DIRE.png\"\n",
    "fig1.savefig(png_hdw, dpi=240); plt.close(fig1)\n",
    "\n",
    "# SF\n",
    "fig2 = plt.figure(figsize=(12, max(5.0, 0.36*len(M_SF.index)+2.2)))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "_draw(ax2, M_SF, list(M_SF.index), list(M_SF.columns),\n",
    "      \"Short-term physical risk (PEAK) — DIRE / SF (storms + flood)\")\n",
    "fig2.tight_layout()\n",
    "png_sf = OUTPNG / \"total_loss__SF__regionsxsectors__peak__DIRE.png\"\n",
    "fig2.savefig(png_sf, dpi=240); plt.close(fig2)\n",
    "\n",
    "# Combined PDF\n",
    "pdf = OUTPDF / \"total_loss__HDW_and_SF__regionsxsectors__peak__DIRE.pdf\"\n",
    "with PdfPages(pdf) as pp:\n",
    "    for png in [png_hdw, png_sf]:\n",
    "        img = plt.imread(png)\n",
    "        fig = plt.figure(figsize=(12,9)); plt.imshow(img); plt.axis('off')\n",
    "        pp.savefig(fig); plt.close(fig)\n",
    "\n",
    "print(\"[DIRE] PNG saved:\", png_hdw)\n",
    "print(\"[DIRE] PNG saved:\", png_sf)\n",
    "print(\"[DIRE] PDF saved:\", pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6ec4df-54f8-454b-b120-fd58961f6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _to_long_from_matrix(mat_df: pd.DataFrame, index_name=\"Region\", col_name=\"Sector\", value_name=\"Value\"):\n",
    "    idx = mat_df.index.name or index_name\n",
    "    return (mat_df.reset_index()\n",
    "            .rename(columns={idx: index_name})\n",
    "            .melt(id_vars=index_name, var_name=col_name, value_name=value_name))\n",
    "\n",
    "def write_heatmap_tables(base_outdir: Path, tag: str, mat_df: pd.DataFrame):\n",
    "    data_dir = _ensure_dir(base_outdir / \"data\")\n",
    "    (data_dir / f\"{tag}_heatmap_data.csv\").write_text(mat_df.to_csv())\n",
    "    _to_long_from_matrix(mat_df).to_csv(data_dir / f\"{tag}_heatmap_data_long.csv\", index=False)\n",
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def _save_png_pdf_from_matrix(mat_df, png_path, pdf_path, title, vmin, vmax):\n",
    "    rows, cols = list(mat_df.index), list(mat_df.columns)\n",
    "    Mclip = np.clip(mat_df.to_numpy(float), vmin, vmax)\n",
    "\n",
    "    # PNG\n",
    "    fig, ax = plt.subplots(figsize=(1.2*len(cols), 0.48*len(rows)))\n",
    "    im = ax.imshow(Mclip, aspect=\"auto\", cmap=\"RdYlGn_r\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.set_xticks(range(len(cols))); ax.set_xticklabels(cols, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(rows))); ax.set_yticklabels(rows)\n",
    "    ax.set_xticks(np.arange(-.5, len(cols), 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, len(rows), 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linewidth=0.5, alpha=0.25)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    fig.colorbar(im, ax=ax).set_label(\"Average physical loss (HDW & SF)\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # one-page PDF\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        fig2, ax2 = plt.subplots(figsize=(1.2*len(cols), 0.48*len(rows)))\n",
    "        im2 = ax2.imshow(Mclip, aspect=\"auto\", cmap=\"RdYlGn_r\", vmin=vmin, vmax=vmax)\n",
    "        ax2.set_title(title, pad=10)\n",
    "        ax2.set_xticks(range(len(cols))); ax2.set_xticklabels(cols, rotation=45, ha=\"right\")\n",
    "        ax2.set_yticks(range(len(rows))); ax2.set_yticklabels(rows)\n",
    "        ax2.set_xticks(np.arange(-.5, len(cols), 1), minor=True)\n",
    "        ax2.set_yticks(np.arange(-.5, len(rows), 1), minor=True)\n",
    "        ax2.grid(which=\"minor\", color=\"black\", linewidth=0.5, alpha=0.25)\n",
    "        ax2.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "        fig2.colorbar(im2, ax=ax2).set_label(\"Average physical loss (HDW & SF)\")\n",
    "        fig2.tight_layout()\n",
    "        pdf.savefig(fig2, bbox_inches=\"tight\")\n",
    "        plt.close(fig2)\n",
    "def _set_outdirs(model_tag: str):\n",
    "    root = Path.home() / \"Documents\" / \"Summer_intern\" / \"outputs\" / f\"physical_shortterm_{model_tag}\"\n",
    "    out_png = root / \"png\"; out_pdf = root / \"pdf\"\n",
    "    out_png.mkdir(parents=True, exist_ok=True); out_pdf.mkdir(parents=True, exist_ok=True)\n",
    "    return out_png, out_pdf\n",
    "\n",
    "def _ensure_cols(df):\n",
    "    needed = [\"macro_region\", \"sector_canon\"]\n",
    "    for c in needed:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"Missing column '{c}' in DataFrame.\")\n",
    "    hdw = next((c for c in [\"HDW\",\"hdw\",\"HDW_peak\",\"HDW_sum\"] if c in df.columns), None)\n",
    "    sf  = next((c for c in [\"SF\",\"sf\",\"SF_peak\",\"SF_sum\"] if c in df.columns), None)\n",
    "    if hdw is None or sf is None:\n",
    "        raise RuntimeError(\"Can't find HDW/SF columns.\")\n",
    "    return hdw, sf\n",
    "\n",
    "def _infer_orders(df, region_order=None, sector_order=None):\n",
    "    if region_order is None:\n",
    "        region_order = sorted(df[\"macro_region\"].dropna().unique().tolist())\n",
    "    if sector_order is None:\n",
    "        sector_order = sorted(df[\"sector_canon\"].dropna().unique().tolist())\n",
    "    return region_order, sector_order\n",
    "\n",
    "def _clip_p95(arr):\n",
    "    finite = arr[np.isfinite(arr)]\n",
    "    if finite.size == 0: return arr, 0.0, 1.0\n",
    "    p95 = np.percentile(finite, 95)\n",
    "    vmax = float(p95 if p95 > 0 else np.nanmax(finite))\n",
    "    if not np.isfinite(vmax) or vmax <= 0: vmax = 1.0\n",
    "    return np.clip(arr, 0.0, vmax), 0.0, vmax\n",
    "\n",
    "def _avg_heatmap(df_macro, model_tag: str, title: str, region_order=None, sector_order=None):\n",
    "    \"\"\"\n",
    "    Build the average (HDW + SF)/2 Region×Sector matrix, save PNG/PDF, and write CSV wide+long.\n",
    "    \"\"\"\n",
    "    OUTPNG, OUTPDF = _set_outdirs(model_tag)\n",
    "\n",
    "    # columns & orders\n",
    "    hdw_col, sf_col = _ensure_cols(df_macro)\n",
    "    region_order, sector_order = _infer_orders(df_macro, region_order, sector_order)\n",
    "\n",
    "    # average storyline\n",
    "    df_avg = df_macro.copy()\n",
    "    df_avg[\"ALL\"] = (df_avg[hdw_col].astype(float) + df_avg[sf_col].astype(float)) / 2.0\n",
    "\n",
    "    # drop non-present labels to avoid white rows/cols\n",
    "    present_regions = set(df_avg[\"macro_region\"].unique())\n",
    "    present_sectors = set(df_avg[\"sector_canon\"].unique())\n",
    "    rows = [r for r in (region_order or []) if r in present_regions] or sorted(present_regions)\n",
    "    cols = [s for s in (sector_order or []) if s in present_sectors] or sorted(present_sectors)\n",
    "    miss_r = [r for r in (region_order or []) if r not in present_regions]\n",
    "    miss_s = [s for s in (sector_order or []) if s not in present_sectors]\n",
    "    if miss_r: print(f\"[WARN:{model_tag}] Dropping missing regions from plot: {miss_r}\")\n",
    "    if miss_s: print(f\"[WARN:{model_tag}] Dropping missing sectors from plot: {miss_s}\")\n",
    "\n",
    "    # pivot to Region×Sector\n",
    "    mat = (df_avg.pivot(index=\"macro_region\", columns=\"sector_canon\", values=\"ALL\")\n",
    "                  .reindex(index=rows, columns=cols))\n",
    "    if mat.isna().all().all():\n",
    "        raise RuntimeError(f\"[{model_tag}] Averaged matrix is all NaN after reindex.\")\n",
    "\n",
    "    # NEW: write CSVs for Streamlit\n",
    "    _ensure_dir(OUTPNG.parent / \"data\")\n",
    "    write_heatmap_tables(OUTPNG.parent, f\"{model_tag}_Average_HDW_SF\", mat)\n",
    "\n",
    "    # common scale (p95), then render\n",
    "    Mclip, vmin, vmax = _clip_p95(mat.to_numpy(float))\n",
    "    png_path = OUTPNG / f\"{model_tag}_Average_HDW_SF.png\"\n",
    "    pdf_path = OUTPDF / f\"{model_tag}_Average_HDW_SF.pdf\"\n",
    "    _save_png_pdf_from_matrix(mat, png_path, pdf_path, title=title, vmin=vmin, vmax=vmax)\n",
    "    print(f\"[WRITE:{model_tag}] PNG -> {png_path}\")\n",
    "    print(f\"[WRITE:{model_tag}] PDF -> {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce27ad83-f1b2-4632-a99f-27993816a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE:DAPS] PNG -> /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/png/DAPS_Average_HDW_SF.png\n",
      "[WRITE:DAPS] PDF -> /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/pdf/DAPS_Average_HDW_SF.pdf\n",
      "[WRITE:DIRE] PNG -> /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/png/DIRE_Average_HDW_SF.png\n",
      "[WRITE:DIRE] PDF -> /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/pdf/DIRE_Average_HDW_SF.pdf\n"
     ]
    }
   ],
   "source": [
    "# === FINAL BLOCK: One averaged heatmap per model (DAPS & DiRe) ===\n",
    "# Paste after: (i) you built DAPS macro table (macro_block4) and DiRe macro table (macro_aug),\n",
    "# and (ii) any region_order / sector_order you may have defined.\n",
    "\n",
    "import os, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Resolve macro DataFrames ----------\n",
    "# DAPS macro with EU-15/EU-27 means (from your Block 4A)\n",
    "try:\n",
    "    macro_daps = macro_block4.copy()\n",
    "except NameError:\n",
    "    macro_daps = None\n",
    "\n",
    "# DiRe macro table (from your DiRe pipeline)\n",
    "try:\n",
    "    macro_dire = macro_aug.copy()\n",
    "except NameError:\n",
    "    macro_dire = None\n",
    "\n",
    "# ---------- Optional: use your fixed orders if defined, else infer later ----------\n",
    "try:\n",
    "    _region_order = list(region_order)\n",
    "except NameError:\n",
    "    _region_order = None\n",
    "\n",
    "try:\n",
    "    _sector_order = list(sector_order)\n",
    "except NameError:\n",
    "    _sector_order = None\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _set_outdirs(model_tag: str):\n",
    "    root = Path.home() / \"Documents\" / \"Summer_intern\" / \"outputs\" / f\"physical_shortterm_{model_tag}\"\n",
    "    out_png = root / \"png\"; out_pdf = root / \"pdf\"\n",
    "    out_png.mkdir(parents=True, exist_ok=True); out_pdf.mkdir(parents=True, exist_ok=True)\n",
    "    return out_png, out_pdf\n",
    "\n",
    "def _ensure_cols(df):\n",
    "    needed = [\"macro_region\", \"sector_canon\"]\n",
    "    for c in needed:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"Missing column '{c}' in DataFrame. Has: {list(df.columns)[:25]} ...\")\n",
    "    # Accept common alternatives for the storylines\n",
    "    hdw_candidates = [\"HDW\",\"hdw\",\"HDW_peak\",\"HDW_sum\"]\n",
    "    sf_candidates  = [\"SF\",\"sf\",\"SF_peak\",\"SF_sum\"]\n",
    "    hdw = next((c for c in hdw_candidates if c in df.columns), None)\n",
    "    sf  = next((c for c in sf_candidates  if c in df.columns), None)\n",
    "    if hdw is None or sf is None:\n",
    "        raise RuntimeError(\n",
    "            \"Can't find HDW/SF columns.\\n\"\n",
    "            f\"Tried HDW in {hdw_candidates}, SF in {sf_candidates}.\\n\"\n",
    "            f\"Available: {list(df.columns)[:25]} ...\"\n",
    "        )\n",
    "    return hdw, sf\n",
    "\n",
    "def _infer_orders(df, region_order=None, sector_order=None):\n",
    "    if region_order is None:\n",
    "        region_order = sorted(df[\"macro_region\"].dropna().unique().tolist())\n",
    "    if sector_order is None:\n",
    "        sector_order = sorted(df[\"sector_canon\"].dropna().unique().tolist())\n",
    "    return region_order, sector_order\n",
    "\n",
    "def _clip_p95(arr):\n",
    "    finite = arr[np.isfinite(arr)]\n",
    "    if finite.size == 0:\n",
    "        return arr, 0.0, 1.0\n",
    "    p95 = np.percentile(finite, 95)\n",
    "    vmax = float(p95 if p95 > 0 else np.nanmax(finite))\n",
    "    if not np.isfinite(vmax) or vmax <= 0:\n",
    "        vmax = 1.0\n",
    "    return np.clip(arr, 0.0, vmax), 0.0, vmax\n",
    "\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if macro_daps is not None and len(macro_daps):\n",
    "    _avg_heatmap(\n",
    "        df_macro=macro_daps,\n",
    "        model_tag=\"DAPS\",\n",
    "        title=\"DAPS — Average of HDW & SF (peak 2023–2030)\",\n",
    "        region_order=_region_order,\n",
    "        sector_order=_sector_order\n",
    "    )\n",
    "else:\n",
    "    print(\"[SKIP] DAPS: macro_block4 not found or empty.\")\n",
    "\n",
    "if macro_dire is not None and len(macro_dire):\n",
    "    _avg_heatmap(\n",
    "        df_macro=macro_dire,\n",
    "        model_tag=\"DIRE\",\n",
    "        title=\"DiRe — Average of HDW & SF (peak 2023–2030)\",\n",
    "        region_order=_region_order,\n",
    "        sector_order=_sector_order\n",
    "    )\n",
    "else:\n",
    "    print(\"[SKIP] DiRe: macro_aug not found or empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce2911d-a20e-43a9-958e-d2f9508163b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] NGFS sectors found (50):\n",
      " - Advanced Electric Appliances\n",
      " - Advanced Heating and Cooking Appliances\n",
      " - Agriculture\n",
      " - Air transport\n",
      " - Basic pharmaceutical products\n",
      " - Batteries\n",
      " - Biofuels\n",
      " - Biomass\n",
      " - Biomass Solid\n",
      " - CCS coal\n",
      " - CO2 Capture\n",
      " - CSS Bio\n",
      " - CSS Gas\n",
      " - Chemical Products\n",
      " - Clean Gas\n",
      " - Coal\n",
      " - Coal fired\n",
      " - Computer, electronic and optical products\n",
      " - Construction\n",
      " - Consumer Goods Industries\n",
      " - Crude Oil\n",
      " - EV Transport Equipment\n",
      " - Equipment for CCS power technology\n",
      " - Equipment for PV panels\n",
      " - Equipment for wind power technology\n",
      " - Fabricated Metal products\n",
      " - Ferrous metals\n",
      " - Gas\n",
      " - Gas Fired\n",
      " - Geothermal\n",
      " - Hydro electric\n",
      " - Hydrogen\n",
      " - Land transport\n",
      " - Market Services\n",
      " - Non Market Services\n",
      " - Non-ferrous metals\n",
      " - Non-metallic minerals\n",
      " - Nuclear\n",
      " - Oil\n",
      " - Oil Fired\n",
      " - Other Equipment Goods\n",
      " - PV\n",
      " - Paper products, publishing\n",
      " - Power Supply\n",
      " - R&D\n",
      " - Rubber and plastic products\n",
      " - Transport equipment (excluding EV)\n",
      " - Warehousing\n",
      " - Water transport\n",
      " - Wind\n",
      "\n",
      "[WARN] Unmatched sectors (add aliases):\n",
      " - Advanced Electric Appliances\n",
      " - Advanced Heating and Cooking Appliances\n",
      " - Batteries\n",
      " - Biofuels\n",
      " - CO2 Capture\n",
      " - CSS Bio\n",
      " - CSS Gas\n",
      " - Clean Gas\n",
      " - Consumer Goods Industries\n",
      " - Gas\n",
      " - Gas Fired\n",
      " - Market Services\n",
      " - Non Market Services\n",
      " - Oil\n",
      " - Oil Fired\n",
      " - Other Equipment Goods\n",
      " - R&D\n",
      "\n",
      "[INFO] Rows per (scenario, model, 10cat) present in the workbook:\n",
      "  scenario          model         Proposed_10cat  rows\n",
      "DAPS_AFR_R direct_impacts            Agriculture  1824\n",
      "DAPS_AFR_R direct_impacts           Construction   608\n",
      "DAPS_AFR_R direct_impacts         Consumer Goods   608\n",
      "DAPS_AFR_R direct_impacts Electricity production  4256\n",
      "DAPS_AFR_R direct_impacts        Equipment Goods  3648\n",
      "DAPS_AFR_R direct_impacts       Heavy Industries  4256\n",
      "DAPS_AFR_R direct_impacts                 Mining  2432\n",
      "DAPS_AFR_R direct_impacts              Transport  1824\n",
      "DAPS_AFR_R direct_impacts            Warehousing   608\n",
      " DAPS_ASIA direct_impacts            Agriculture  5184\n",
      " DAPS_ASIA direct_impacts           Construction  1728\n",
      " DAPS_ASIA direct_impacts         Consumer Goods  1728\n",
      " DAPS_ASIA direct_impacts Electricity production 12096\n",
      " DAPS_ASIA direct_impacts        Equipment Goods 10368\n",
      " DAPS_ASIA direct_impacts       Heavy Industries 12096\n",
      " DAPS_ASIA direct_impacts                 Mining  6912\n",
      " DAPS_ASIA direct_impacts              Transport  5184\n",
      " DAPS_ASIA direct_impacts            Warehousing  1728\n",
      "  DAPS_EUR direct_impacts            Agriculture 18792\n",
      "  DAPS_EUR direct_impacts           Construction  6264\n",
      "  DAPS_EUR direct_impacts         Consumer Goods  6264\n",
      "  DAPS_EUR direct_impacts Electricity production 43848\n",
      "  DAPS_EUR direct_impacts        Equipment Goods 37584\n",
      "  DAPS_EUR direct_impacts       Heavy Industries 43848\n",
      "  DAPS_EUR direct_impacts                 Mining 25056\n",
      "  DAPS_EUR direct_impacts              Transport 18792\n",
      "  DAPS_EUR direct_impacts            Warehousing  6264\n",
      "  DAPS_NAM direct_impacts            Agriculture  1944\n",
      "  DAPS_NAM direct_impacts           Construction   648\n",
      "  DAPS_NAM direct_impacts         Consumer Goods   648\n",
      "  DAPS_NAM direct_impacts Electricity production  4536\n",
      "  DAPS_NAM direct_impacts        Equipment Goods  3888\n",
      "  DAPS_NAM direct_impacts       Heavy Industries  4536\n",
      "  DAPS_NAM direct_impacts                 Mining  2592\n",
      "  DAPS_NAM direct_impacts              Transport  1944\n",
      "  DAPS_NAM direct_impacts            Warehousing   648\n",
      "  DAPS_OCE direct_impacts            Agriculture   648\n",
      "  DAPS_OCE direct_impacts           Construction   216\n",
      "  DAPS_OCE direct_impacts         Consumer Goods   216\n",
      "  DAPS_OCE direct_impacts Electricity production  1512\n",
      "  DAPS_OCE direct_impacts        Equipment Goods  1296\n",
      "  DAPS_OCE direct_impacts       Heavy Industries  1512\n",
      "  DAPS_OCE direct_impacts                 Mining   864\n",
      "  DAPS_OCE direct_impacts              Transport   648\n",
      "  DAPS_OCE direct_impacts            Warehousing   216\n",
      "  DAPS_SAM direct_impacts            Agriculture  1236\n",
      "  DAPS_SAM direct_impacts           Construction   412\n",
      "  DAPS_SAM direct_impacts         Consumer Goods   412\n",
      "  DAPS_SAM direct_impacts Electricity production  2884\n",
      "  DAPS_SAM direct_impacts        Equipment Goods  2472\n",
      "  DAPS_SAM direct_impacts       Heavy Industries  2884\n",
      "  DAPS_SAM direct_impacts                 Mining  1648\n",
      "  DAPS_SAM direct_impacts              Transport  1236\n",
      "  DAPS_SAM direct_impacts            Warehousing   412\n",
      "      DiRe direct_impacts            Agriculture 29556\n",
      "      DiRe direct_impacts           Construction  9852\n",
      "      DiRe direct_impacts         Consumer Goods  9852\n",
      "      DiRe direct_impacts Electricity production 68964\n",
      "      DiRe direct_impacts        Equipment Goods 59112\n",
      "      DiRe direct_impacts       Heavy Industries 68964\n",
      "      DiRe direct_impacts                 Mining 39408\n",
      "      DiRe direct_impacts              Transport 29556\n",
      "      DiRe direct_impacts            Warehousing  9852\n",
      "\n",
      "[WRITE] Crosswalk CSV -> /Users/noenotter/Documents/Summer_intern/ngfs_sector_to_10cat_crosswalk.csv\n"
     ]
    }
   ],
   "source": [
    "# ====== NGFS short-term sector discovery + crosswalk to your 10 categories ======\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.home() / \"Documents\" / \"Summer_intern\"\n",
    "XLSX = ROOT / \"NGFS_shortterm_FULL_timeseries.xlsx\"\n",
    "\n",
    "# Prefer these 4 families; fallback to whatever exists\n",
    "PREF_SHEETS = [\"capital_destruction\",\"production_lost\",\"productivity_loss\",\"labour_productivity_loss\"]\n",
    "\n",
    "def load_ngfs_shortterm(xlsx_path: Path):\n",
    "    xls = pd.ExcelFile(xlsx_path)\n",
    "    sheets = [s for s in PREF_SHEETS if s in xls.sheet_names] or xls.sheet_names\n",
    "    frames = []\n",
    "    for sh in sheets:\n",
    "        # Read only columns we need; be permissive about names\n",
    "        df = pd.read_excel(xlsx_path, sheet_name=sh)\n",
    "        df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "        # Normalize minimal set\n",
    "        need = [\"family\",\"sector\",\"model\",\"scenario\",\"region\",\"unit\",\"year\",\"value\",\"hazard\"]\n",
    "        for c in need:\n",
    "            if c not in df.columns: df[c] = np.nan\n",
    "        if df[\"family\"].isna().all(): df[\"family\"] = sh\n",
    "        frames.append(df[need])\n",
    "    full = pd.concat(frames, ignore_index=True)\n",
    "    return full\n",
    "\n",
    "full = load_ngfs_shortterm(XLSX)\n",
    "\n",
    "# Keep only the physical-risk model (\"direct_impacts\") if present\n",
    "mask_direct = full[\"model\"].astype(str).str.lower().eq(\"direct_impacts\")\n",
    "if mask_direct.any():\n",
    "    full = full[mask_direct].copy()\n",
    "\n",
    "# 1) List sectors present\n",
    "ngfs_sectors = sorted(full[\"sector\"].dropna().astype(str).unique())\n",
    "print(f\"[INFO] NGFS sectors found ({len(ngfs_sectors)}):\")\n",
    "for s in ngfs_sectors:\n",
    "    print(\" -\", s)\n",
    "\n",
    "# 2) Crosswalk → your 10 categories (keyword rules, extend as needed)\n",
    "CATS = {\n",
    "    \"Agriculture\": [\"agric\", \"biomass\", \"forestry\", \"fishing\", \"aquaculture\"],\n",
    "    \"Mining\": [\"coal\", \"lignite\", \"crude\", \"petroleum\", \"natural gas\", \"oil & gas\", \"metal ore\", \"mining\", \"quarry\"],\n",
    "    \"Consumer Goods\": [\"food\", \"beverage\", \"tobacco\", \"textile\", \"apparel\", \"garment\", \"leather\", \"wood\", \"paper\", \"printing\", \"publish\"],\n",
    "    \"Heavy Industries\": [\"coke\", \"refined\", \"petroleum\", \"chemical\", \"pharm\", \"rubber\", \"plastic\",\n",
    "                         \"non-metallic\", \"cement\", \"glass\", \"basic metal\", \"ferrous\", \"iron\", \"steel\",\n",
    "                         \"non-ferrous\", \"fabricated metal\"],\n",
    "    \"Equipment Goods\": [\"computer\", \"electronic\", \"optical\", \"electrical equip\", \"machinery\",\n",
    "                        \"motor vehicle\", \"transport equipment\", \"furniture\", \"other manufacturing\",\n",
    "                        \"repair\", \"installation\", \"battery\", \"pv\", \"solar\", \"wind\", \"ccs\"],\n",
    "    \"Electricity production\": [\"electricity\", \"power\", \"gas supply\", \"steam\", \"air conditioning\",\n",
    "                               \"hydrogen\", \"nuclear\", \"biomass\", \"hydro\", \"geothermal\"],\n",
    "    \"Construction\": [\"construct\", \"civil engineering\", \"special\"],\n",
    "    \"Transport\": [\"transport\", \"land\", \"road\", \"rail\", \"pipeline\", \"water\", \"maritime\", \"air\", \"aviation\"],\n",
    "    \"Warehousing\": [\"warehous\", \"postal\", \"courier\", \"support activit\"],\n",
    "    \"Services\": [\"accommodation\", \"food service\", \"financial\", \"insurance\", \"auxiliary\",\n",
    "                 \"real estate\", \"information\", \"telecom\", \"publishing\", \"it \", \" r&d\", \"research\",\n",
    "                 \"legal\", \"accounting\", \"consult\", \"engineering\", \"scientific\", \"advertis\",\n",
    "                 \"veterinary\", \"leasing\", \"employment\", \"travel agency\", \"security\",\n",
    "                 \"building service\", \"admin\", \"membership\", \"repair of computers\",\n",
    "                 \"personal service\", \"households as employers\", \"extraterritorial\",\n",
    "                 \"education\", \"public administration\", \"health\", \"social work\",\n",
    "                 \"waste\", \"water supply\", \"sewerage\", \"remediation\", \"gambling\", \"arts\", \"sports\",\n",
    "                 \"wholesale\", \"retail\"]\n",
    "}\n",
    "\n",
    "def to_category(label: str) -> str | None:\n",
    "    lab = str(label).lower()\n",
    "    # Exact helpful heuristics first\n",
    "    if \"electric\" in lab and \"supply\" in lab: return \"Electricity production\"\n",
    "    if \"power\" in lab: return \"Electricity production\"\n",
    "    # Generic keyword scan\n",
    "    for cat, keys in CATS.items():\n",
    "        for k in keys:\n",
    "            if k in lab:\n",
    "                return cat\n",
    "    return None\n",
    "\n",
    "crosswalk = pd.DataFrame({\"NGFS_sector_raw\": ngfs_sectors})\n",
    "crosswalk[\"Proposed_10cat\"] = crosswalk[\"NGFS_sector_raw\"].apply(to_category)\n",
    "\n",
    "unmatched = crosswalk[crosswalk[\"Proposed_10cat\"].isna()].copy()\n",
    "if len(unmatched):\n",
    "    print(\"\\n[WARN] Unmatched sectors (add aliases):\")\n",
    "    for s in unmatched[\"NGFS_sector_raw\"]:\n",
    "        print(\" -\", s)\n",
    "else:\n",
    "    print(\"\\n[OK] All NGFS sectors mapped to a 10-category.\")\n",
    "\n",
    "# 3) If you want an averaged-by-10cat view ready for heatmaps:\n",
    "def collapse_to_10cat(df: pd.DataFrame, crosswalk_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Join category on sector\n",
    "    cw = crosswalk_df.dropna(subset=[\"Proposed_10cat\"]).copy()\n",
    "    df2 = df.merge(cw, left_on=\"sector\", right_on=\"NGFS_sector_raw\", how=\"left\")\n",
    "    # Keep only rows we can place\n",
    "    df2 = df2[~df2[\"Proposed_10cat\"].isna()].copy()\n",
    "    # Example aggregation: total_loss by country×10cat×hazard×year (like your pipeline upstream)\n",
    "    # Here we just show you what categories are present per scenario/model\n",
    "    present = (df2.groupby([\"scenario\",\"model\",\"Proposed_10cat\"], dropna=False)\n",
    "                  .size().reset_index(name=\"rows\"))\n",
    "    return df2, present\n",
    "\n",
    "collapsed_df, present_table = collapse_to_10cat(full, crosswalk)\n",
    "\n",
    "print(\"\\n[INFO] Rows per (scenario, model, 10cat) present in the workbook:\")\n",
    "print(present_table.sort_values([\"scenario\",\"model\",\"Proposed_10cat\"]).to_string(index=False))\n",
    "\n",
    "# If you want to export the crosswalk for manual touch-up:\n",
    "crosswalk_path = ROOT / \"ngfs_sector_to_10cat_crosswalk.csv\"\n",
    "crosswalk.to_csv(crosswalk_path, index=False)\n",
    "print(f\"\\n[WRITE] Crosswalk CSV -> {crosswalk_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c54429-9bac-48c8-bd32-e487d6a582b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE:DAPS] 10-category heatmaps saved to: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/png_10cats and /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/pdf_10cats\n",
      "[WRITE:DAPS] 10-category data saved to: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DAPS/data_10cats\n",
      "[WRITE:DiRe] 10-category heatmaps saved to: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/png_10cats and /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/pdf_10cats\n",
      "[WRITE:DiRe] 10-category data saved to: /Users/noenotter/Documents/Summer_intern/outputs/physical_shortterm_DIRE/data_10cats\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# BLOCK X — 10-category sector heatmaps (mean across subsectors), DAPS & DiRe\n",
    "# Paste AFTER your existing blocks (after the DiRe pipeline + average block).\n",
    "# =============================\n",
    "\n",
    "import pandas as pd, numpy as np, unicodedata\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# ---- 10 categories in your desired order (Overleaf Table A.2) ----\n",
    "TEN_ORDER = [\n",
    "    \"Agriculture\",\"Mining\",\"Consumer Goods\",\"Heavy Industries\",\"Equipment Goods\",\n",
    "    \"Electricity production\",\"Construction\",\"Transport\",\"Warehousing\",\"Services\",\n",
    "]\n",
    "\n",
    "# ---- Explicit NGFS subsector -> 10-category mapping (covers your labels) ----\n",
    "NGFS_TO_10CAT = {\n",
    "    # Agriculture\n",
    "    \"Agriculture\":\"Agriculture\",\"Biomass\":\"Agriculture\",\"Biomass Solid\":\"Agriculture\",\n",
    "    # Mining / fuels upstream\n",
    "    \"Coal\":\"Mining\",\"Crude Oil\":\"Mining\",\"Oil\":\"Mining\",\"Gas\":\"Mining\",\"Clean Gas\":\"Mining\",\n",
    "    # Consumer Goods\n",
    "    \"Consumer Goods Industries\":\"Consumer Goods\",\"Paper products, publishing\":\"Consumer Goods\",\n",
    "    # Heavy Industries\n",
    "    \"Chemical Products\":\"Heavy Industries\",\"Basic pharmaceutical products\":\"Heavy Industries\",\n",
    "    \"Rubber and plastic products\":\"Heavy Industries\",\"Non-metallic minerals\":\"Heavy Industries\",\n",
    "    \"Ferrous metals\":\"Heavy Industries\",\"Non-ferrous metals\":\"Heavy Industries\",\n",
    "    \"Fabricated Metal products\":\"Heavy Industries\",\n",
    "    # Equipment Goods\n",
    "    \"Computer, electronic and optical products\":\"Equipment Goods\",\n",
    "    \"Advanced Electric Appliances\":\"Equipment Goods\",\n",
    "    \"Advanced Heating and Cooking Appliances\":\"Equipment Goods\",\n",
    "    \"Transport equipment (excluding EV)\":\"Equipment Goods\",\n",
    "    \"EV Transport Equipment\":\"Equipment Goods\",\"Other Equipment Goods\":\"Equipment Goods\",\n",
    "    \"Batteries\":\"Equipment Goods\",\"Equipment for PV panels\":\"Equipment Goods\",\n",
    "    \"Equipment for wind power technology\":\"Equipment Goods\",\n",
    "    \"Equipment for CCS power technology\":\"Equipment Goods\",\n",
    "    # Electricity production\n",
    "    \"Power Supply\":\"Electricity production\",\"Hydro electric\":\"Electricity production\",\n",
    "    \"Wind\":\"Electricity production\",\"PV\":\"Electricity production\",\"Geothermal\":\"Electricity production\",\n",
    "    \"Nuclear\":\"Electricity production\",\"Coal fired\":\"Electricity production\",\n",
    "    \"Oil Fired\":\"Electricity production\",\"Gas Fired\":\"Electricity production\",\n",
    "    \"Biofuels\":\"Electricity production\",\"Hydrogen\":\"Electricity production\",\n",
    "    \"CO2 Capture\":\"Electricity production\",\"CCS coal\":\"Electricity production\",\n",
    "    \"CSS Gas\":\"Electricity production\",\"CSS Bio\":\"Electricity production\",\n",
    "    # Construction\n",
    "    \"Construction\":\"Construction\",\n",
    "    # Transport\n",
    "    \"Land transport\":\"Transport\",\"Water transport\":\"Transport\",\"Air transport\":\"Transport\",\n",
    "    # Warehousing\n",
    "    \"Warehousing\":\"Warehousing\",\n",
    "    # Services\n",
    "    \"Market Services\":\"Services\",\"Non Market Services\":\"Services\",\"R&D\":\"Services\",\n",
    "}\n",
    "\n",
    "# ---- Small helpers (re-use existing ones if already defined globally) ----\n",
    "def _normstrong(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = s.replace(\"\\u00A0\",\" \").replace(\"\\u200B\",\"\").replace(\"\\u200C\",\"\").replace(\"\\u200D\",\"\")\n",
    "    return \" \".join(s.split()).strip()\n",
    "\n",
    "_alias = {\"United States\":\"USA\",\"U.S.A.\":\"USA\",\"US\":\"USA\",\"United States of America\":\"USA\",\n",
    "          \"Korea, Republic of\":\"South Korea\",\"Republic of Korea\":\"South Korea\",\n",
    "          \"Russian Federation\":\"Russia\",\"Viet Nam\":\"Vietnam\"}\n",
    "\n",
    "def _macro_aggregate_with_eu_means_10cat(peaks_df: pd.DataFrame, MACRO_CSV: Path) -> pd.DataFrame:\n",
    "    mm = pd.read_csv(MACRO_CSV, dtype=str)\n",
    "    mm[\"country\"] = mm[\"country\"].astype(str).map(_normstrong).replace(_alias).map(_normstrong)\n",
    "    mm[\"macro_region\"] = mm[\"macro_region\"].astype(str).map(_normstrong)\n",
    "\n",
    "    essentials = {\n",
    "        \"USA\":\"USA\",\"Mexico\":\"Mexico\",\n",
    "        \"Australia\":\"Australia_NZ\",\"New Zealand\":\"Australia_NZ\",\n",
    "        \"Switzerland\":\"Europe_Non_EU\",\"Norway\":\"Europe_Non_EU\",\"Iceland\":\"Europe_Non_EU\",\"Turkey\":\"Europe_Non_EU\",\n",
    "        \"Oceania\":\"Australia_NZ\",\"Rest of Europe\":\"Europe_Non_EU\",\n",
    "        \"Rest of Energy Producing Countries\":\"Other\",\"Rest of World\":\"Other\",\n",
    "    }\n",
    "    for ctry, macro in essentials.items():\n",
    "        if not (mm[\"country\"] == ctry).any():\n",
    "            mm.loc[len(mm)] = {\"country\": ctry, \"macro_region\": macro}\n",
    "\n",
    "    dfm = peaks_df.copy()\n",
    "    dfm[\"country\"] = dfm[\"country\"].astype(str).map(_normstrong).replace(_alias).map(_normstrong)\n",
    "    dfm = dfm.merge(mm, on=\"country\", how=\"left\", validate=\"many_to_one\")\n",
    "    if dfm[\"macro_region\"].isna().any():\n",
    "        missing = sorted(dfm.loc[dfm[\"macro_region\"].isna(),\"country\"].unique())\n",
    "        print(\"[NOTE] Auto-assigning to 'Other':\", missing)\n",
    "        dfm.loc[dfm[\"macro_region\"].isna(), \"macro_region\"] = \"Other\"\n",
    "\n",
    "    # Macro aggregation\n",
    "    macro = (dfm.groupby([\"macro_region\",\"sector_10cat\"], dropna=False)\n",
    "               .agg(HDW=(\"HDW\",\"sum\"), SF=(\"SF\",\"sum\")).reset_index())\n",
    "\n",
    "    # Force 'Other' = mean of its members\n",
    "    other_mean = (dfm.loc[dfm[\"macro_region\"]==\"Other\"]\n",
    "                    .groupby([\"sector_10cat\"], dropna=False)\n",
    "                    .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\")).reset_index())\n",
    "    if not other_mean.empty:\n",
    "        other_mean.insert(0, \"macro_region\", \"Other\")\n",
    "        macro = pd.concat([macro[macro[\"macro_region\"]!=\"Other\"], other_mean], ignore_index=True)\n",
    "\n",
    "    # EU15 / EU27 means\n",
    "    EU15 = [\"Austria\",\"Belgium\",\"Denmark\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Ireland\",\"Italy\",\n",
    "            \"Luxembourg\",\"Netherlands\",\"Portugal\",\"Spain\",\"Sweden\",\"United Kingdom\"]\n",
    "    EU27 = [\"Austria\",\"Belgium\",\"Bulgaria\",\"Croatia\",\"Cyprus\",\"Czechia\",\"Denmark\",\"Estonia\",\"Finland\",\n",
    "            \"France\",\"Germany\",\"Greece\",\"Hungary\",\"Ireland\",\"Italy\",\"Latvia\",\"Lithuania\",\"Luxembourg\",\n",
    "            \"Malta\",\"Netherlands\",\"Poland\",\"Portugal\",\"Romania\",\"Slovakia\",\"Slovenia\",\"Spain\",\"Sweden\"]\n",
    "\n",
    "    def _eu_mean(peaks_country_df, members, name):\n",
    "        used = [c for c in members if c in set(peaks_country_df[\"country\"])]\n",
    "        if not used: return None\n",
    "        sub = peaks_country_df[peaks_country_df[\"country\"].isin(used)]\n",
    "        out = (sub.groupby([\"sector_10cat\"], dropna=False)\n",
    "                  .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\")).reset_index())\n",
    "        out.insert(0, \"macro_region\", name)\n",
    "        return out\n",
    "\n",
    "    eu15 = _eu_mean(dfm, EU15, \"EU-15\"); eu27 = _eu_mean(dfm, EU27, \"EU-27\")\n",
    "    macro_aug = macro.copy()\n",
    "    if \"EU-27\" in set(macro_aug[\"macro_region\"]):\n",
    "        macro_aug = macro_aug[macro_aug[\"macro_region\"] != \"EU-27\"]\n",
    "    for extra in (eu15, eu27):\n",
    "        if extra is not None:\n",
    "            macro_aug = pd.concat([macro_aug, extra], ignore_index=True)\n",
    "\n",
    "    macro_aug = (macro_aug\n",
    "                 .groupby([\"macro_region\",\"sector_10cat\"], as_index=False)\n",
    "                 .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\")))\n",
    "    return macro_aug\n",
    "\n",
    "def _collapse_to_10cat_and_peaks(df_src: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Equal-weight collapse of NGFS subsectors into 10 categories, then PEAKs & storylines.\"\"\"\n",
    "    d = df_src.copy()\n",
    "    d.columns = [str(c).strip().lower() for c in d.columns]\n",
    "    for c in [\"country\",\"sector\",\"hazard\",\"year\",\"value\",\"scenario\",\"model\",\"region\"]:\n",
    "        if c not in d.columns: d[c] = np.nan\n",
    "\n",
    "    d[\"sector\"] = d[\"sector\"].astype(str)\n",
    "    d[\"sector_10cat\"] = d[\"sector\"].map(NGFS_TO_10CAT)\n",
    "    if d[\"sector_10cat\"].isna().any():\n",
    "        missing_labels = sorted(d.loc[d[\"sector_10cat\"].isna(),\"sector\"].unique().tolist())\n",
    "        print(\"[WARN] Unmapped sector labels dropped:\", missing_labels)\n",
    "        d = d[~d[\"sector_10cat\"].isna()].copy()\n",
    "\n",
    "    g = (d.groupby([\"country\",\"sector_10cat\",\"hazard\",\"year\"], dropna=False)[\"value\"]\n",
    "           .mean().reset_index(name=\"total_loss\"))\n",
    "\n",
    "    base = g[[\"country\",\"sector_10cat\",\"year\"]].drop_duplicates()\n",
    "    haz_list = [\"coastal_flood\",\"river_flood\",\"floods\",\"storms\",\"tropical_cyclone\",\"winter_storm\",\n",
    "                \"heatwave\",\"drought\",\"wildfire\"]\n",
    "    for hz in haz_list:\n",
    "        sub = g[g[\"hazard\"].eq(hz)][[\"country\",\"sector_10cat\",\"year\",\"total_loss\"]]\n",
    "        base = base.merge(sub.rename(columns={\"total_loss\": f\"tot_{hz}\"}),\n",
    "                          on=[\"country\",\"sector_10cat\",\"year\"], how=\"left\")\n",
    "\n",
    "    def _rowmax(cols):\n",
    "        arrs = [base[c].to_numpy() for c in cols if c in base]\n",
    "        if not arrs: return np.full(len(base), np.nan)\n",
    "        A = np.vstack(arrs).T\n",
    "        return np.nanmax(A, axis=1)\n",
    "\n",
    "    base[\"tot_flood\"]   = _rowmax([\"tot_floods\",\"tot_river_flood\",\"tot_coastal_flood\"])\n",
    "    base[\"tot_stormsC\"] = _rowmax([\"tot_storms\",\"tot_tropical_cyclone\",\"tot_winter_storm\"])\n",
    "\n",
    "    peaks = (base.groupby([\"country\",\"sector_10cat\"], dropna=False)\n",
    "                  .agg(peak_storms=(\"tot_stormsC\",\"max\"),\n",
    "                       peak_flood=(\"tot_flood\",\"max\"),\n",
    "                       peak_heat=(\"tot_heatwave\",\"max\"),\n",
    "                       peak_drought=(\"tot_drought\",\"max\"),\n",
    "                       peak_wildfire=(\"tot_wildfire\",\"max\")).reset_index())\n",
    "\n",
    "    for col in [\"peak_storms\",\"peak_flood\",\"peak_heat\",\"peak_drought\",\"peak_wildfire\"]:\n",
    "        if col not in peaks:\n",
    "            peaks[col] = np.nan\n",
    "    \n",
    "    peaks[\"HDW\"] = peaks[[\"peak_heat\",\"peak_drought\",\"peak_wildfire\"]].fillna(0).sum(axis=1)\n",
    "    peaks[\"SF\"]  = peaks[[\"peak_storms\",\"peak_flood\"]].fillna(0).sum(axis=1)\n",
    "\n",
    "    # 🚫 remove Taiwan here\n",
    "    peaks[\"country\"] = peaks[\"country\"].astype(str).str.strip()\n",
    "    peaks = peaks[~peaks[\"country\"].str.fullmatch(\"Taiwan\", case=False)].copy()\n",
    "\n",
    "    return peaks\n",
    "\n",
    "def _plot_heatmap(Mdf: pd.DataFrame, title, out_png, out_pdf):\n",
    "    vals = Mdf.to_numpy(dtype=float).ravel()\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    vmax = float(np.percentile(vals,95)) if vals.size else 1.0\n",
    "    vmin = 0.0\n",
    "\n",
    "    rows, cols = list(Mdf.index), list(Mdf.columns)\n",
    "    fig, ax = plt.subplots(figsize=(1.2*len(cols), 0.48*len(rows)))\n",
    "    im = ax.imshow(Mdf.to_numpy(float), aspect=\"auto\", cmap=\"RdYlGn_r\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_xticks(range(len(cols))); ax.set_xticklabels(cols, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(rows))); ax.set_yticklabels(rows)\n",
    "    ax.set_xticks(np.arange(-0.5, len(cols), 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, len(rows), 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linewidth=0.4)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    c = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04,\n",
    "                     format=FuncFormatter(lambda v,_: f\"{v:.1f}\"))\n",
    "    c.ax.set_ylabel(\"Exposure (units of input)\", rotation=270, va=\"bottom\")\n",
    "    ax.set_title(title, pad=10)\n",
    "    fig.tight_layout(); fig.savefig(out_png, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "    with PdfPages(out_pdf) as pdf:\n",
    "        fig2, ax2 = plt.subplots(figsize=(1.2*len(cols), 0.48*len(rows)))\n",
    "        im2 = ax2.imshow(Mdf.to_numpy(float), aspect=\"auto\", cmap=\"RdYlGn_r\", vmin=vmin, vmax=vmax)\n",
    "        ax2.set_xticks(range(len(cols))); ax2.set_xticklabels(cols, rotation=45, ha=\"right\")\n",
    "        ax2.set_yticks(range(len(rows))); ax2.set_yticklabels(rows)\n",
    "        ax2.set_xticks(np.arange(-0.5, len(cols), 1), minor=True)\n",
    "        ax2.set_yticks(np.arange(-0.5, len(rows), 1), minor=True)\n",
    "        ax2.grid(which=\"minor\", color=\"black\", linewidth=0.4)\n",
    "        ax2.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "        c2 = plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04,\n",
    "                          format=FuncFormatter(lambda v,_: f\"{v:.1f}\"))\n",
    "        c2.ax.set_ylabel(\"Exposure (units of input)\", rotation=270, va=\"bottom\")\n",
    "        ax2.set_title(title, pad=10)\n",
    "        fig2.tight_layout(); pdf.savefig(fig2, bbox_inches=\"tight\"); plt.close(fig2)\n",
    "\n",
    "def _run_model_10cat(df_src, model_tag: str, out_root_tag: str,\n",
    "                     region_order=None, sector10_order=None):\n",
    "    if df_src is None or df_src.empty:\n",
    "        print(f\"[SKIP] {model_tag}: no source data found.\")\n",
    "        return\n",
    "\n",
    "    # 1) collapse to 10-cat & compute HDW/SF peaks per country\n",
    "    peaks_10 = _collapse_to_10cat_and_peaks(df_src)\n",
    "\n",
    "    # 2) aggregate to macro with EU means\n",
    "    macro10 = _macro_aggregate_with_eu_means_10cat(peaks_10, MACRO_CSV)\n",
    "\n",
    "    # 3) determine orders\n",
    "    if region_order is None:\n",
    "        region_order = sorted(macro10[\"macro_region\"].unique().tolist())\n",
    "    if sector10_order is None:\n",
    "        sector10_order = [c for c in TEN_ORDER if c in set(macro10[\"sector_10cat\"])]\n",
    "\n",
    "    # 4) matrices as DataFrames\n",
    "    def _mat_df(val):\n",
    "        return (macro10.pivot(index=\"macro_region\", columns=\"sector_10cat\", values=val)\n",
    "                       .reindex(index=region_order, columns=sector10_order))\n",
    "\n",
    "    M10_HDW_df = _mat_df(\"HDW\")\n",
    "    M10_SF_df  = _mat_df(\"SF\")\n",
    "\n",
    "    # 5) out dirs\n",
    "    ROOT = Path.home() / \"Documents\" / \"Summer_intern\" / \"outputs\" / f\"physical_shortterm_{out_root_tag}\"\n",
    "    OUTPNG = ROOT / \"png_10cats\"; OUTPDF = ROOT / \"pdf_10cats\"; OUTDATA = ROOT / \"data_10cats\"\n",
    "    OUTPNG.mkdir(parents=True, exist_ok=True); OUTPDF.mkdir(parents=True, exist_ok=True); OUTDATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 6) plots\n",
    "    _plot_heatmap(M10_HDW_df, f\"{model_tag} — 10 categories — HDW (peak 2023–2030)\",\n",
    "                  OUTPNG / f\"{model_tag}__HDW__10cats__peak.png\",\n",
    "                  OUTPDF / f\"{model_tag}__HDW__10cats__peak.pdf\")\n",
    "    _plot_heatmap(M10_SF_df, f\"{model_tag} — 10 categories — SF (peak 2023–2030)\",\n",
    "                  OUTPNG / f\"{model_tag}__SF__10cats__peak.png\",\n",
    "                  OUTPDF / f\"{model_tag}__SF__10cats__peak.pdf\")\n",
    "\n",
    "    # 7) write CSVs\n",
    "    def _to_long(df: pd.DataFrame):\n",
    "        idx = df.index.name or \"Region\"\n",
    "        return (df.reset_index()\n",
    "                .rename(columns={idx:\"Region\"})\n",
    "                .melt(id_vars=\"Region\", var_name=\"Sector\", value_name=\"Value\"))\n",
    "\n",
    "    (OUTDATA / \"HDW_10cats_peak_heatmap_data.csv\").write_text(M10_HDW_df.to_csv())\n",
    "    _to_long(M10_HDW_df).to_csv(OUTDATA / \"HDW_10cats_peak_heatmap_data_long.csv\", index=False)\n",
    "    (OUTDATA / \"SF_10cats_peak_heatmap_data.csv\").write_text(M10_SF_df.to_csv())\n",
    "    _to_long(M10_SF_df).to_csv(OUTDATA / \"SF_10cats_peak_heatmap_data_long.csv\", index=False)\n",
    "\n",
    "    # 8) Average (HDW & SF)\n",
    "    M10_AVG_df = (M10_HDW_df.add(M10_SF_df, fill_value=0.0)) / 2.0\n",
    "    _plot_heatmap(M10_AVG_df, f\"{model_tag} — 10 categories — Average (HDW & SF)\",\n",
    "                  OUTPNG / f\"{model_tag}__AVG_HDW_SF__10cats.png\",\n",
    "                  OUTPDF / f\"{model_tag}__AVG_HDW_SF__10cats.pdf\")\n",
    "    (OUTDATA / \"AVG_HDW_SF_10cats_heatmap_data.csv\").write_text(M10_AVG_df.to_csv())\n",
    "    _to_long(M10_AVG_df).to_csv(OUTDATA / \"AVG_HDW_SF_10cats_heatmap_data_long.csv\", index=False)\n",
    "\n",
    "    print(f\"[WRITE:{model_tag}] 10-category heatmaps saved to: {OUTPNG} and {OUTPDF}\")\n",
    "    print(f\"[WRITE:{model_tag}] 10-category data saved to: {OUTDATA}\")\n",
    "\n",
    "# ---- Source datasets ----\n",
    "try:\n",
    "    df_src_daps = df_block1.copy()\n",
    "except NameError:\n",
    "    df_src_daps = None\n",
    "\n",
    "try:\n",
    "    _full0 = full0.copy()\n",
    "except NameError:\n",
    "    def _rebuild_full0(_IN_XLSX):\n",
    "        FAMILY_SHEETS = [\"capital_destruction\",\"production_lost\",\"productivity_loss\",\"labour_productivity_loss\"]\n",
    "        parts = []\n",
    "        for sh in FAMILY_SHEETS:\n",
    "            df = pd.read_excel(_IN_XLSX, sheet_name=sh,\n",
    "                               usecols=[\"family\",\"hazard\",\"sector\",\"model\",\"scenario\",\"region\",\"unit\",\"year\",\"value\"])\n",
    "            df.columns = [c.strip().lower() for c in df.columns]\n",
    "            if \"family\" not in df.columns: \n",
    "                df[\"family\"] = sh\n",
    "            parts.append(df)\n",
    "        f0 = pd.concat(parts, ignore_index=True)\n",
    "        f0[\"year\"]  = pd.to_numeric(f0[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        f0[\"value\"] = pd.to_numeric(f0[\"value\"], errors=\"coerce\")\n",
    "        f0 = f0[f0[\"year\"].between(2023, 2030)]\n",
    "        f0 = f0[f0[\"model\"].fillna(\"\").str.lower().eq(\"direct_impacts\")].copy()\n",
    "        f0[\"country\"] = f0[\"region\"].astype(str).str.split(\" - \").str[0].str.strip()\n",
    "        return f0\n",
    "    _full0 = _rebuild_full0(IN_XLSX)\n",
    "\n",
    "df_src_dire = _full0[_full0[\"scenario\"].astype(str).str.strip().str.lower().eq(\"dire\")].copy()\n",
    "\n",
    "# ---- Run for DAPS and DiRe ----\n",
    "try:\n",
    "    _CANON_REGIONS = _region_order  # reuse if already defined upstream\n",
    "except NameError:\n",
    "    _CANON_REGIONS = None\n",
    "\n",
    "_run_model_10cat(df_src_daps, \"DAPS\", \"DAPS\", region_order=_CANON_REGIONS, sector10_order=TEN_ORDER)\n",
    "_run_model_10cat(df_src_dire, \"DiRe\",  \"DIRE\", region_order=_CANON_REGIONS, sector10_order=TEN_ORDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9656546-7bf0-4ced-8365-132129eb3cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OTHER] Top contributing countries (original 10 sectors):\n",
      "     country       HDW       SF\n",
      "Saudi Arabia 43.605519 4.574846\n",
      "      Turkey 27.636146 4.279562\n",
      "\n",
      "[OTHER] Shares within 'Other' (%):\n",
      "     country  HDW_share_%  SF_share_%\n",
      "Saudi Arabia    61.207889   51.667439\n",
      "      Turkey    38.792111   48.332561\n",
      "\n",
      "[SCALES] Original 10 sectors — DAPS:\n",
      "[SCALE] DAPS HDW: p95=12.051, max=34.224\n",
      "[SCALE] DAPS SF: p95=4.624, max=5.391\n",
      "\n",
      "[CHECK] India–Construction (original 10 sectors):\n",
      "             HDW   SF\n",
      "count   1.000000  1.0\n",
      "mean   27.221689  0.0\n",
      "std          NaN  NaN\n",
      "min    27.221689  0.0\n",
      "50%    27.221689  0.0\n",
      "max    27.221689  0.0\n",
      "\n",
      "[OTHER] Top contributing countries (10-category DAPS):\n",
      "     country       HDW       SF\n",
      "Saudi Arabia 10.719337 1.724093\n",
      "      Turkey  5.425557 1.612812\n",
      "[WARN] 10-category scale/India checks skipped: name '_macro_aggregate_with_eu_means' is not defined\n",
      "[WARN] Combined check skipped: name 'M10_HDW' is not defined\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# DIAGNOSTICS: \"Other\" breakdown, scales, and India-Construction checks\n",
    "# Run this AFTER your existing blocks (the ones that define peaks_block3, macro_block4, macro_aug, etc.)\n",
    "# ===============================\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Helper: p95\n",
    "def _p95_np(A):\n",
    "    z = A[np.isfinite(A)]\n",
    "    return float(np.percentile(z, 95)) if z.size else 1.0\n",
    "\n",
    "# --- 0) Which countries ended up in \"Other\" (original 10-sector pipeline)?\n",
    "try:\n",
    "    # Recreate the country→macro mapping used earlier\n",
    "    macro_map = pd.read_csv(MACRO_CSV, dtype=str)  # defined in your code\n",
    "    # Normalize like Block 4 did\n",
    "    def _norm(s: str) -> str:\n",
    "        import unicodedata\n",
    "        s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "        s = s.replace(\"\\u00A0\",\" \").replace(\"\\u200B\",\"\").replace(\"\\u200C\",\"\").replace(\"\\u200D\",\"\")\n",
    "        return \" \".join(s.split()).strip()\n",
    "    alias = {\"United States\":\"USA\",\"U.S.A.\":\"USA\",\"US\":\"USA\",\"United States of America\":\"USA\",\n",
    "             \"Korea, Republic of\":\"South Korea\",\"Republic of Korea\":\"South Korea\",\n",
    "             \"Russian Federation\":\"Russia\",\"Viet Nam\":\"Vietnam\"}\n",
    "    macro_map[\"country\"] = macro_map[\"country\"].astype(str).map(_norm).replace(alias).map(_norm)\n",
    "    macro_map[\"macro_region\"] = macro_map[\"macro_region\"].astype(str).map(_norm)\n",
    "\n",
    "    # peaks_block3 = country-level peaks from your original 10-sector pipeline\n",
    "    peaks_countries = peaks_block3.copy()\n",
    "    peaks_countries[\"country\"] = peaks_countries[\"country\"].astype(str).map(_norm).replace(alias).map(_norm)\n",
    "    peaks_join = peaks_countries.merge(macro_map, on=\"country\", how=\"left\")\n",
    "\n",
    "    other_cty = (peaks_join[peaks_join[\"macro_region\"] == \"Other\"]\n",
    "                 .groupby(\"country\", as_index=False)[[\"HDW\",\"SF\"]].sum()\n",
    "                 .sort_values([\"HDW\",\"SF\"], ascending=False))\n",
    "    print(\"\\n[OTHER] Top contributing countries (original 10 sectors):\")\n",
    "    print(other_cty.head(10).to_string(index=False))\n",
    "\n",
    "    # Optional: share within 'Other'\n",
    "    total_other = other_cty[[\"HDW\",\"SF\"]].sum()\n",
    "    if total_other.sum() > 0:\n",
    "        tmp = other_cty.copy()\n",
    "        tmp[\"HDW_share_%\"] = 100 * tmp[\"HDW\"] / (total_other[\"HDW\"] if total_other[\"HDW\"] else 1)\n",
    "        tmp[\"SF_share_%\"]  = 100 * tmp[\"SF\"]  / (total_other[\"SF\"]  if total_other[\"SF\"]  else 1)\n",
    "        print(\"\\n[OTHER] Shares within 'Other' (%):\")\n",
    "        print(tmp.head(10)[[\"country\",\"HDW_share_%\",\"SF_share_%\"]].to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not compute 'Other' breakdown for original pipeline:\", e)\n",
    "\n",
    "# --- 1) Matrix scales (original 10-sector pipeline)\n",
    "try:\n",
    "    # M_HDW and M_SF already built in your Block 5 for both DAPS and DiRe\n",
    "    def _matrix_stats(M, label):\n",
    "        A = M.to_numpy(float) if hasattr(M, \"to_numpy\") else np.array(M, float)\n",
    "        print(f\"[SCALE] {label}: p95={_p95_np(A):.3f}, max={np.nanmax(A):.3f}\")\n",
    "    print(\"\\n[SCALES] Original 10 sectors — DAPS:\")\n",
    "    _matrix_stats(M_HDW, \"DAPS HDW\")\n",
    "    _matrix_stats(M_SF,  \"DAPS SF\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Original 10-sector scale check skipped:\", e)\n",
    "\n",
    "# --- 2) India – Construction comparison (original pipeline)\n",
    "try:\n",
    "    india_constr = peaks_countries[(peaks_countries[\"country\"]==\"India\") &\n",
    "                                   (peaks_countries[\"sector_canon\"]==\"Construction\")]\n",
    "    print(\"\\n[CHECK] India–Construction (original 10 sectors):\")\n",
    "    print(india_constr[[\"HDW\",\"SF\"]].describe(percentiles=[]))\n",
    "except Exception as e:\n",
    "    print(\"[WARN] India–Construction (original) check skipped:\", e)\n",
    "\n",
    "# === 10-category pipeline diagnostics (uses functions from your BLOCK X) ===\n",
    "try:\n",
    "    # Rebuild 10-category country peaks from the same source DAPS Block 1\n",
    "    # df_block1 = your per-country chosen DAPS data\n",
    "    from math import isfinite\n",
    "    peaks_10_daps = _collapse_to_10cat_and_peaks(df_block1)\n",
    "    peaks_10_dire = _collapse_to_10cat_and_peaks(full0[full0[\"scenario\"].astype(str).str.lower().eq(\"dire\")])\n",
    "\n",
    "    # 'Other' breakdown for 10-categories (DAPS)\n",
    "    mm = pd.read_csv(MACRO_CSV, dtype=str)\n",
    "    mm[\"country\"] = mm[\"country\"].astype(str).map(_norm).replace(alias).map(_norm)\n",
    "    mm[\"macro_region\"] = mm[\"macro_region\"].astype(str).map(_norm)\n",
    "    pj = peaks_10_daps.copy()\n",
    "    pj[\"country\"] = pj[\"country\"].astype(str).map(_norm).replace(alias).map(_norm)\n",
    "    pj = pj.merge(mm, on=\"country\", how=\"left\")\n",
    "    other10 = (pj[pj[\"macro_region\"]==\"Other\"]\n",
    "               .groupby(\"country\", as_index=False)[[\"HDW\",\"SF\"]].sum()\n",
    "               .sort_values([\"HDW\",\"SF\"], ascending=False))\n",
    "    print(\"\\n[OTHER] Top contributing countries (10-category DAPS):\")\n",
    "    print(other10.head(10).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"[WARN] 10-category 'Other' breakdown skipped:\", e)\n",
    "\n",
    "# --- 3) Build 10-category macro matrices and scale stats (uses your helper)\n",
    "try:\n",
    "    macro10_daps = _macro_aggregate_with_eu_means(peaks_10_daps, MACRO_CSV)\n",
    "    rows = [r for r in region_order if r in set(macro10_daps[\"macro_region\"])]\n",
    "    cols = [c for c in TEN_ORDER if c in set(macro10_daps[\"sector_10cat\"])]\n",
    "    M10_HDW = macro10_daps.pivot(index=\"macro_region\", columns=\"sector_10cat\", values=\"HDW\").reindex(index=rows, columns=cols)\n",
    "    M10_SF  = macro10_daps.pivot(index=\"macro_region\", columns=\"sector_10cat\", values=\"SF\" ).reindex(index=rows, columns=cols)\n",
    "    print(\"\\n[SCALES] 10-category — DAPS:\")\n",
    "    _matrix_stats(M10_HDW, \"DAPS HDW (10-cat)\")\n",
    "    _matrix_stats(M10_SF,  \"DAPS SF  (10-cat)\")\n",
    "\n",
    "    # India–Construction check in 10-cat (same name)\n",
    "    india_constr_10 = peaks_10_daps[(peaks_10_daps[\"country\"]==\"India\") &\n",
    "                                    (peaks_10_daps[\"sector_10cat\"]==\"Construction\")]\n",
    "    print(\"\\n[CHECK] India–Construction (10 categories):\")\n",
    "    print(india_constr_10[[\"HDW\",\"SF\"]].describe(percentiles=[]))\n",
    "except Exception as e:\n",
    "    print(\"[WARN] 10-category scale/India checks skipped:\", e)\n",
    "\n",
    "# --- 4) Optional: choose how to compute \"combined\" (mean vs sum)\n",
    "COMBINED_MODE = \"mean\"  # change to \"sum\" to align with Eric's suggestion\n",
    "def combine_storylines(H, S, mode=\"mean\"):\n",
    "    A = np.array(H, float); B = np.array(S, float)\n",
    "    if mode == \"sum\":  return A + B\n",
    "    return 0.5*(A + B)\n",
    "\n",
    "try:\n",
    "    M10_AVG = combine_storylines(M10_HDW.to_numpy(float), M10_SF.to_numpy(float), mode=COMBINED_MODE)\n",
    "    print(f\"\\n[COMBINED] Mode={COMBINED_MODE}: p95={_p95_np(M10_AVG):.3f}, max={np.nanmax(M10_AVG):.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Combined check skipped:\", e)\n",
    "\n",
    "# --- 5) (Optional) Recompute 'Other' as MEAN instead of SUM for fairness\n",
    "TRY_OTHER_AS_MEAN = False\n",
    "if TRY_OTHER_AS_MEAN:\n",
    "    # Recompute macro10_daps but averaging within each macro (including Other)\n",
    "    pj = peaks_10_daps.merge(mm, on=\"country\", how=\"left\")\n",
    "    macro10_mean = (pj.groupby([\"macro_region\",\"sector_10cat\"], dropna=False)\n",
    "                      .agg(HDW=(\"HDW\",\"mean\"), SF=(\"SF\",\"mean\")).reset_index())\n",
    "    M10m_HDW = macro10_mean.pivot(\"macro_region\",\"sector_10cat\",\"HDW\").reindex(index=rows, columns=cols)\n",
    "    print(\"\\n[SCALES] 10-category — DAPS with macro means everywhere (incl. Other):\")\n",
    "    _matrix_stats(M10m_HDW, \"DAPS HDW (10-cat, macro means)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
